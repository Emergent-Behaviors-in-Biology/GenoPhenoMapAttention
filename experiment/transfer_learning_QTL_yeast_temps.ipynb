{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7189319-9f83-4f11-b913-81bda0a81bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pickle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbe7d6-9068-4838-a747-9f657f24e481",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d027bd5d-8171-44d2-b97b-f79ede71b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load genotype data\n",
    "geno_data = np.load('merged_geno_data.npy')\n",
    "\n",
    "# List of phenotype data files corresponding to different temperature environments\n",
    "pheno_files = ['pheno_data_23C.txt', 'pheno_data_25C.txt', 'pheno_data_27C.txt', \n",
    "               'pheno_data_30C.txt', 'pheno_data_33C.txt', 'pheno_data_35C.txt', 'pheno_data_37C.txt']\n",
    "\n",
    "# Shuffle the genotype data to randomize the order of segregants\n",
    "num_segregants = geno_data.shape[0]\n",
    "shuffled_indices = list(range(num_segregants))\n",
    "random.seed(0)  # Use a fixed seed for reproducibility\n",
    "random.shuffle(shuffled_indices)\n",
    "geno_data = geno_data[shuffled_indices]\n",
    "\n",
    "# Load causal loci indices\n",
    "ind_loci_list = np.load('ind_loci_list_3.npy')\n",
    "\n",
    "# Initialize an empty list to store fitness values for different environments\n",
    "fitness_list = []\n",
    "\n",
    "# Load phenotype (fitness) data for each environment and apply the same shuffled indices\n",
    "for file in pheno_files:\n",
    "    df_pheno = pd.read_csv(file, sep=\"\\t\")  # Load phenotype data from a tab-separated file\n",
    "    data_fitness = df_pheno.iloc[shuffled_indices, 1].to_numpy()  # Extract fitness values\n",
    "    fitness_list.append(data_fitness)\n",
    "\n",
    "# Select only the causal loci from genotype data and convert [0,1] to [-1,+1]\n",
    "geno_data = 2.0 * geno_data[:, sorted(ind_loci_list)] - 1.0\n",
    "\n",
    "# Reshape the fitness array to match (num_segregants, num_environments)\n",
    "fitness = np.array(fitness_list).T  # Shape becomes (99950, 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be760bb-0cf6-4d3a-abbe-e545f246acbc",
   "metadata": {},
   "source": [
    "### Attention layer class in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6bcec0-fe17-4fae-bffa-ec22978b2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerAttention(nn.Module):\n",
    "    def __init__(self, input_dim, query_dim, key_dim, seq_length):\n",
    "        \"\"\"\n",
    "        Implements a three-layer attention model\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimension of the input features.\n",
    "            query_dim (int): Dimension of the query matrix.\n",
    "            key_dim (int): Dimension of the key matrix.\n",
    "            seq_length (int): Length of the input sequence.\n",
    "        \"\"\"\n",
    "        super(ThreeLayerAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.key_dim = key_dim\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Learnable weight matrices for the first attention layer\n",
    "        self.query_matrix_1 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_1 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_1 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "\n",
    "        # Learnable weight matrices for the second attention layer\n",
    "        self.query_matrix_2 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_2 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_2 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "        \n",
    "        # Learnable weight matrices for the third attention layer\n",
    "        self.query_matrix_3 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_3 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_3 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "        \n",
    "        # Learnable random projection matrix (reduces input dimensionality)\n",
    "        self.random_matrix = nn.Parameter(torch.empty(seq_length - 1, low_dim))\n",
    "        \n",
    "        # Learnable coefficients for attended values\n",
    "        self.coeffs_attended = nn.Parameter(torch.empty(seq_length, input_dim))\n",
    "\n",
    "        # Learnable scalar offset for output adjustment\n",
    "        self.offset = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "        # Initialize model parameters\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"\n",
    "        Initializes model parameters using a normal distribution with a small standard deviation.\n",
    "        \"\"\"\n",
    "        init_scale = 0.04  # Standard deviation for parameter initialization\n",
    "\n",
    "        # Initialize all weight matrices and learnable parameters\n",
    "        for param in [self.query_matrix_1, self.key_matrix_1, self.value_matrix_1,\n",
    "                      self.query_matrix_2, self.key_matrix_2, self.value_matrix_2,\n",
    "                      self.query_matrix_3, self.key_matrix_3, self.value_matrix_3,\n",
    "                      self.random_matrix, self.coeffs_attended, self.offset]:\n",
    "            init.normal_(param, std=init_scale)\n",
    "\n",
    "    def forward(self, x, envs_one_hot):\n",
    "        \"\"\"\n",
    "        Forward pass through the three-layer self-attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Genotype input tensor of shape (batch_size, seq_length-1, input_dim).\n",
    "            envs_one_hot (Tensor): One-hot encoded environmental condition of shape (batch_size, num_envs).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Final attended output of shape (batch_size,).\n",
    "        \"\"\"\n",
    "        # Apply random projection to the genotype data\n",
    "        y = torch.matmul(x, self.random_matrix)\n",
    "\n",
    "        # Concatenate environmental encoding\n",
    "        z = torch.cat((y, envs_one_hot.unsqueeze(1)), dim=1)\n",
    "\n",
    "        # Add a bias term (constant ones)\n",
    "        z = torch.cat((z, torch.ones(z.shape[0], z.shape[1], 1).to(device)), dim=2)\n",
    "\n",
    "        # First self-attention layer\n",
    "        query_1 = torch.matmul(z, self.query_matrix_1)\n",
    "        key_1 = torch.matmul(z, self.key_matrix_1)\n",
    "        value_1 = torch.matmul(z, self.value_matrix_1)\n",
    "        scores_1 = torch.matmul(query_1, key_1.transpose(1, 2)).softmax(dim=-1)  # Compute attention scores\n",
    "        attended_values_1 = torch.matmul(scores_1, value_1)  # Apply attention weights\n",
    "\n",
    "        # Second self-attention layer\n",
    "        query_2 = torch.matmul(attended_values_1, self.query_matrix_2)\n",
    "        key_2 = torch.matmul(attended_values_1, self.key_matrix_2)\n",
    "        value_2 = torch.matmul(attended_values_1, self.value_matrix_2)\n",
    "        scores_2 = torch.matmul(query_2, key_2.transpose(1, 2)).softmax(dim=-1)\n",
    "        attended_values_2 = torch.matmul(scores_2, value_2)\n",
    "\n",
    "        # Third self-attention layer\n",
    "        query_3 = torch.matmul(attended_values_2, self.query_matrix_3)\n",
    "        key_3 = torch.matmul(attended_values_2, self.key_matrix_3)\n",
    "        value_3 = torch.matmul(attended_values_2, self.value_matrix_3)\n",
    "        scores_3 = torch.matmul(query_3, key_3.transpose(1, 2)).softmax(dim=-1)\n",
    "        attended_values_3 = torch.matmul(scores_3, value_3)\n",
    "        \n",
    "        # Compute final weighted sum using learned coefficients\n",
    "        output = torch.einsum(\"bij,ij->b\", attended_values_3, self.coeffs_attended) + self.offset\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8012e5f3-7a83-47a4-8fba-66b2e19515b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_temp_embeddings(temps):\n",
    "    \"\"\"\n",
    "    Generate one-hot embeddings for temperatures with random shuffling and pad them with zeros.\n",
    "\n",
    "    Args:\n",
    "    - temps (Tensor): A 1D tensor of temperature values.\n",
    "    - low_dim (int): The target dimensionality (default is 30).\n",
    "    - seed (int): The random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: A 2D tensor of shape [len(temps), low_dim] containing padded one-hot embeddings.\n",
    "    \"\"\"\n",
    "    # Define the list of unique temperatures\n",
    "    unique_temps = [23.0, 25.0, 27.0, 30.0, 33.0, 35.0, 37.0]\n",
    "\n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    shuffled_temps = random.sample(unique_temps, len(unique_temps))  # Shuffle the temperatures\n",
    "\n",
    "    # Create a mapping from temperature to one-hot index\n",
    "    temp_to_index = {temp: idx for idx, temp in enumerate(shuffled_temps)}\n",
    "\n",
    "    # Initialize a tensor to hold the one-hot vectors\n",
    "    batch_size = len(temps)\n",
    "    one_hot_vectors = torch.zeros((batch_size, len(unique_temps)))\n",
    "\n",
    "    # Set the appropriate positions to 1 based on the shuffled temperature values\n",
    "    for i, temp in enumerate(temps):\n",
    "        if temp.item() in temp_to_index:\n",
    "            one_hot_vectors[i, temp_to_index[temp.item()]] = 1.0\n",
    "\n",
    "    # Pad with zeros to match low_dim\n",
    "    padding = torch.zeros((batch_size, low_dim - len(unique_temps)))\n",
    "    one_hot_padded = torch.cat((one_hot_vectors, padding), dim=1)\n",
    "\n",
    "    return one_hot_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e40897-0e34-448d-a566-d40779af167a",
   "metadata": {},
   "source": [
    "### Training for different amounts of training data for $T=23$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4035529c-61c1-47bb-af43-e53a326ece0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_to_file(filename, a, b, c):\n",
    "    \"\"\"\n",
    "    Writes training progress metrics to a file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the file where data will be saved.\n",
    "        a (int/float): First value (e.g., low_dim).\n",
    "        b (int): Second value (e.g., epoch number).\n",
    "        c (float): Third value (e.g., validation R² score).\n",
    "    \"\"\"\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(f\"{a} {b} {c}\\n\")\n",
    "\n",
    "# Set computation device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Define model hyperparameters\n",
    "low_dim = 12\n",
    "num_loci = len(ind_loci_list)\n",
    "seq_length = num_loci + 1\n",
    "input_dim = low_dim + 1\n",
    "query_dim = low_dim + 1\n",
    "key_dim = low_dim + 1\n",
    "hidden_dim = 32\n",
    "\n",
    "# Split dataset into training (85%) and testing (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(geno_data, fitness, test_size=0.15, random_state=42)\n",
    "\n",
    "# Define temperature values (used for environment embeddings)\n",
    "temps = torch.tensor([23, 25, 27, 30, 33, 35, 37])\n",
    "\n",
    "# Define a range for training data size `m`.  Here, 1390 represents the number of NaN values in the fitness training data.\n",
    "m_values = np.logspace(np.log10(2), np.log10(len(X_train) - 1390), num=6, dtype=int)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Iterate over different training data sizes \n",
    "for m in m_values:\n",
    "    \n",
    "    save_dir = f\"./transfer_learning_QTL_yeast_temps/num_data_{m}\"\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    filename = f\"{save_dir}/validation_r2.txt\"\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "\n",
    "    # Copy `y_train` to avoid modifying the original data\n",
    "    y_train_modified = y_train.copy()\n",
    "\n",
    "    # Reduce available training data by setting excess values to NaN\n",
    "    for idx in [0]:  # Processing only the first column (adjustable)\n",
    "        total_entries = y_train_modified.shape[0]\n",
    "        current_non_nan_count = total_entries - 1390  # Total - predefined NaN count\n",
    "        additional_nan_needed = current_non_nan_count - m  # Calculate how many to remove\n",
    "\n",
    "        not_nan_indices = np.where(~np.isnan(y_train_modified[:, idx]))[0]\n",
    "        random_indices = np.random.choice(not_nan_indices, size=additional_nan_needed, replace=False)\n",
    "        y_train_modified[random_indices, idx] = np.nan  # Assign NaN\n",
    "\n",
    "    # Split into training and validation sets\n",
    "    X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train_modified, test_size=0.15, random_state=42)\n",
    "\n",
    "    # Compute mean and standard deviation for normalization\n",
    "    mean_values = np.nanmean(y_train2, axis=0)\n",
    "    std_values = np.nanstd(y_train2, axis=0)\n",
    "\n",
    "    # Normalize target fitness data\n",
    "    y_train2 = (y_train2 - mean_values) / std_values\n",
    "    y_val = (y_val - mean_values) / std_values\n",
    "    y_test = (y_test - mean_values) / std_values\n",
    "\n",
    "    # Convert dataset to PyTorch tensors\n",
    "    X_train_tens = torch.tensor(X_train2).float()\n",
    "    y_train_tens = torch.tensor(np.array(y_train2)).float()\n",
    "\n",
    "    # Define batch sizes\n",
    "    num_elements = 9\n",
    "    batch_size = 7 * num_elements\n",
    "    chunk_size = 50\n",
    "    num_epochs = 200\n",
    "    num_batches = X_train_tens.size(0) // batch_size\n",
    "    \n",
    "    # Initialize the attention model and move it to CUDA\n",
    "    attention_layer = ThreeLayerAttention(input_dim, query_dim, key_dim, seq_length).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.Adam(attention_layer.parameters(), lr=0.001)\n",
    "\n",
    "    # TRAINING LOOP\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Shuffle training data at the beginning of each epoch\n",
    "        indices = torch.randperm(X_train_tens.size(0))\n",
    "        train_input_shuffled = X_train_tens[indices]\n",
    "        train_target_shuffled = y_train_tens[indices].T\n",
    "\n",
    "        # Mini-batch training\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "\n",
    "            fitness_env = []\n",
    "\n",
    "            # Collect num_elements number of fitness data from each environment\n",
    "            for i, row in enumerate(train_target_shuffled):\n",
    "                start_col = start_idx + i * num_elements\n",
    "                end_col = start_col + num_elements\n",
    "                fitness_env.extend(row[start_col:end_col])\n",
    "\n",
    "            mini_batch_input = train_input_shuffled[start_idx:end_idx]\n",
    "            mini_batch_target = torch.tensor(fitness_env)\n",
    "\n",
    "            # Identify and remove NaN values in fitness data    \n",
    "            nan_mask = np.isnan(mini_batch_target)\n",
    "            nan_indices = np.where(nan_mask)[0]\n",
    "\n",
    "            # Generate one-hot encoded temperature embeddings\n",
    "            envs = temps.repeat(num_elements)     \n",
    "            envs = [envs[i] for i in range(len(envs)) if i not in nan_indices]\n",
    "            envs = torch.tensor(envs)\n",
    "            envs_one_hot = one_hot_temp_embeddings(envs).to(device)        \n",
    "\n",
    "            # Remove corresponding NaN values from input and target data\n",
    "            mini_batch_input = np.delete(mini_batch_input, nan_indices, axis=0).to(device)\n",
    "            mini_batch_target = np.delete(mini_batch_target, nan_indices).to(device)\n",
    "\n",
    "            # Create one-hot genotype representation\n",
    "            one_hot_mini_batch_input = torch.zeros((mini_batch_input.shape[0], num_loci, num_loci), device=device)\n",
    "            indices = torch.arange(num_loci, device=device)\n",
    "            one_hot_mini_batch_input[:, indices, indices] = mini_batch_input.squeeze()\n",
    "\n",
    "            # Forward pass\n",
    "            train_output = attention_layer(one_hot_mini_batch_input, envs_one_hot).to(device)\n",
    "            train_loss = loss_function(train_output, mini_batch_target)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Save model parameters after each epoch\n",
    "        model_state_path = os.path.join(save_dir, f\"epoch_{epoch}.pt\")\n",
    "        torch.save(attention_layer.state_dict(), model_state_path)\n",
    "\n",
    "        # VALIDATION EVALUATION\n",
    "        y_pred = torch.tensor([]).to(device)\n",
    "        y_val_all = np.array([])\n",
    "\n",
    "        for env in range(7):\n",
    "            y_val_env = y_val.T[env]\n",
    "\n",
    "            # Identify and remove NaN values in validation data\n",
    "            nan_mask = np.isnan(y_val_env)\n",
    "            nan_indices = np.where(nan_mask)[0]\n",
    "\n",
    "            X_val_env = np.delete(X_val, nan_indices, axis=0)\n",
    "            y_val_env = np.delete(y_val_env, nan_indices, axis=0)\n",
    "\n",
    "            # Convert validation dataset to PyTorch tensors\n",
    "            X_val_tens = torch.tensor(np.array(X_val_env)).float().to(device)\n",
    "\n",
    "            # Process validation data in chunks\n",
    "            for i in range(0, len(X_val_tens), chunk_size):\n",
    "                chunk = X_val_tens[i:i + chunk_size].to(device)\n",
    "                chunk_size_actual = min(chunk_size, len(X_val_tens) - i)\n",
    "\n",
    "                envs = [temps[env] for _ in range(chunk_size_actual)]\n",
    "                envs = torch.tensor(envs)\n",
    "                envs_one_hot = one_hot_temp_embeddings(envs).to(device)\n",
    "\n",
    "                one_hot_val_input = torch.zeros((chunk_size_actual, num_loci, num_loci), device=device)\n",
    "                indices = torch.arange(num_loci, device=device)\n",
    "                one_hot_val_input[:, indices, indices] = chunk.squeeze(dim=1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    i_pred = attention_layer(one_hot_val_input, envs_one_hot)\n",
    "\n",
    "                y_pred = torch.cat((y_pred, i_pred), dim=0)\n",
    "\n",
    "            y_val_all = np.concatenate((y_val_all, y_val_env))\n",
    "\n",
    "        # Compute validation R² score\n",
    "        val_r_squared = r2_score(y_val_all, y_pred.cpu())\n",
    "\n",
    "        # Save validation performance\n",
    "        write_to_file(filename, low_dim, epoch, val_r_squared)\n",
    "        torch.cuda.empty_cache()  # Clear PyTorch cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8196bc-bb14-4a6d-a7e0-237928e22ca4",
   "metadata": {},
   "source": [
    "### Model prediction performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d88f4e-fb29-4ffa-8a04-ad25d291948c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_to_file(filename, a, b, c):\n",
    "    \"\"\"\n",
    "    Writes data to a text file in space-separated format.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the file where data will be saved.\n",
    "        a (int/float): First value (e.g., number of training data points).\n",
    "        b (int/float): Second value (e.g., temperature condition).\n",
    "        c (float): Third value (e.g., test R² score).\n",
    "    \"\"\"\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(f\"{a} {b} {c}\\n\")\n",
    "\n",
    "# Define the output file for storing number of training samples vs. R² scores\n",
    "filename2 = \"./transfer_learning_QTL_yeast_temps/num_data_vs_r2_score.txt\"\n",
    "\n",
    "# Remove the file if it exists (to start fresh)\n",
    "if os.path.exists(filename2):\n",
    "    os.remove(filename2)\n",
    "\n",
    "# Set computation device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define temperature values used for environmental embeddings\n",
    "temps = torch.tensor([23, 25, 27, 30, 33, 35, 37])\n",
    "\n",
    "# Split the data into training (85%) and testing (15%) datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(geno_data, fitness, test_size=0.15, random_state=42)\n",
    "\n",
    "# Define a range for the training data size m. Here, 1390 represents the number of NaN values in the fitness training data.\n",
    "m_values = np.logspace(np.log10(2), np.log10(len(X_train) - 1390), num=6, dtype=int)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Iterate over different training data sizes \n",
    "for m in m_values:\n",
    "    \n",
    "    # Resplit the dataset to ensure independent splits for each `m`\n",
    "    X_train, X_test, y_train, y_test = train_test_split(geno_data, fitness, test_size=0.15, random_state=42)\n",
    "        \n",
    "    # Create a copy of `y_train` to avoid modifying the original data\n",
    "    y_train_modified = y_train.copy()\n",
    "\n",
    "    # Reduce available training data by setting excess values to NaN\n",
    "    for idx in [0]:  # Processing only the first column (adjustable)\n",
    "        total_entries = y_train_modified.shape[0]\n",
    "        current_non_nan_count = total_entries - 1390  # Total - predefined NaN count\n",
    "        additional_nan_needed = current_non_nan_count - m  # Calculate how many to remove\n",
    "\n",
    "        not_nan_indices = np.where(~np.isnan(y_train_modified[:, idx]))[0]\n",
    "        random_indices = np.random.choice(not_nan_indices, size=additional_nan_needed, replace=False)\n",
    "        y_train_modified[random_indices, idx] = np.nan  # Assign NaN\n",
    "\n",
    "    # Split into training and validation sets\n",
    "    X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train_modified, test_size=0.15, random_state=42)\n",
    "\n",
    "    # Compute mean and standard deviation for normalization\n",
    "    mean_values = np.nanmean(y_train2, axis=0)\n",
    "    std_values = np.nanstd(y_train2, axis=0)\n",
    "    print(m, mean_values[2], std_values[2])  # Print statistics for debugging\n",
    "\n",
    "    # Normalize target fitness data\n",
    "    y_train2 = (y_train2 - mean_values) / std_values\n",
    "    y_val = (y_val - mean_values) / std_values\n",
    "    y_test = (y_test - mean_values) / std_values\n",
    "\n",
    "    # Define model parameters\n",
    "    low_dim = 12\n",
    "    chunk_size = 100\n",
    "    num_loci = len(ind_loci_list)\n",
    "    seq_length = num_loci + 1\n",
    "    input_dim = low_dim + 1\n",
    "    query_dim = low_dim + 1\n",
    "    key_dim = low_dim + 1\n",
    "    hidden_dim = 32\n",
    "\n",
    "    # Find the epoch with the highest validation R² score\n",
    "    filename = f\"./transfer_learning_QTL_yeast_temps/num_data_{m}/validation_r2.txt\"\n",
    "    data = pd.read_csv(filename, sep='\\s+', header=None)\n",
    "    max_row_index = data[2].idxmax()  # Find index of max value in the second column\n",
    "    max_row = data.loc[max_row_index]  # Retrieve the row with the highest R² score\n",
    "    max_second_column_value = max_row[1]  # Extract best epoch number\n",
    "\n",
    "    # Create an instance of the trained ThreeLayerAttention model\n",
    "    attention_layer = ThreeLayerAttention(input_dim, query_dim, key_dim, seq_length).to(device)\n",
    "\n",
    "    # Load the saved model parameters from the best epoch\n",
    "    epoch = int(max_second_column_value)\n",
    "    model_path = f\"./transfer_learning_QTL_yeast_temps/num_data_{m}/epoch_{epoch}.pt\"\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    attention_layer.load_state_dict(state_dict)\n",
    "    attention_layer.to(device)  # Move model to GPU if available\n",
    "    attention_layer.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Iterate over all environmental conditions (temperature values)\n",
    "    for env in range(7):\n",
    "        y_pred_env = torch.tensor([]).to(device)\n",
    "        y_test_env = y_test.T[env]  # Extract test fitness values for the given environment\n",
    "\n",
    "        # Identify and remove NaN values in the test data\n",
    "        nan_mask = np.isnan(y_test_env)\n",
    "        nan_indices = np.where(nan_mask)[0]\n",
    "\n",
    "        # Remove corresponding segregants with NaN values\n",
    "        X_test_env = np.delete(X_test, nan_indices, axis=0)\n",
    "        y_test_env = np.delete(y_test_env, nan_indices, axis=0)\n",
    "\n",
    "        # Convert test genotype data to PyTorch tensors\n",
    "        X_test_tens = torch.tensor(np.array(X_test_env)).float().to(device)\n",
    "\n",
    "        # Process test data in chunks to avoid memory issues\n",
    "        for i in range(0, len(X_test_tens), chunk_size):\n",
    "            chunk = X_test_tens[i:i + chunk_size].to(device)\n",
    "            chunk_size_actual = min(chunk_size, len(X_test_tens) - i)\n",
    "\n",
    "            # Generate one-hot encoded temperature embeddings\n",
    "            envs = torch.tensor([temps[env]] * chunk_size_actual)\n",
    "            envs_one_hot = one_hot_temp_embeddings(envs).to(device)\n",
    "\n",
    "            # Create one-hot encoded genotype representation\n",
    "            one_hot_test_input = torch.zeros((chunk_size_actual, num_loci, num_loci), device=device)\n",
    "            indices = torch.arange(num_loci, device=device)\n",
    "            one_hot_test_input[:, indices, indices] = chunk.squeeze(dim=1)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            with torch.no_grad():\n",
    "                i_pred_env = attention_layer(one_hot_test_input, envs_one_hot)\n",
    "\n",
    "            # Store predictions\n",
    "            y_pred_env = torch.cat((y_pred_env, i_pred_env), dim=0)\n",
    "\n",
    "        # Compute test R² score\n",
    "        test_r_squared = r2_score(y_test_env, y_pred_env.cpu())\n",
    "\n",
    "        # Save results to file\n",
    "        write_to_file(filename2, m, temps[env], test_r_squared)\n",
    "        print(m, temps[env], test_r_squared)  # Print evaluation results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
