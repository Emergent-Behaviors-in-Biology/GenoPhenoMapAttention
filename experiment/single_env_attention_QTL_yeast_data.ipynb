{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7189319-9f83-4f11-b913-81bda0a81bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pickle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbe7d6-9068-4838-a747-9f657f24e481",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d027bd5d-8171-44d2-b97b-f79ede71b7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load genotype data from a NumPy file\n",
    "geno_data = np.load('merged_geno_data.npy')\n",
    "\n",
    "# List of phenotype (fitness) data files corresponding to 18 different environments\n",
    "pheno_files = ['pheno_data_23C.txt', 'pheno_data_25C.txt', 'pheno_data_27C.txt', 'pheno_data_30C.txt', \n",
    "               'pheno_data_33C.txt', 'pheno_data_35C.txt', 'pheno_data_37C.txt', 'pheno_data_cu.txt',\n",
    "               'pheno_data_suloc.txt', 'pheno_data_ynb.txt', 'pheno_data_eth.txt', 'pheno_data_gu.txt', \n",
    "               'pheno_data_li.txt', 'pheno_data_mann.txt', 'pheno_data_mol.txt', 'pheno_data_raff.txt', \n",
    "               'pheno_data_sds.txt', 'pheno_data_4NQO.txt']\n",
    "\n",
    "# Shuffle the genotype data to randomize sample order\n",
    "num_segregants = geno_data.shape[0]  # Total number of segregants (samples)\n",
    "shuffled_indices = list(range(num_segregants))  # Create index list for shuffling\n",
    "random.seed(0)  # Set a fixed seed for reproducibility\n",
    "random.shuffle(shuffled_indices)  # Shuffle the indices\n",
    "geno_data = geno_data[shuffled_indices]  # Apply shuffled indices to genotype data\n",
    "\n",
    "# Load the list of independent loci for feature selection\n",
    "ind_loci_list = np.load('ind_loci_list_3.npy')\n",
    "\n",
    "# Initialize a list to store fitness values across all 18 environments\n",
    "fitness_list = []\n",
    "\n",
    "# Iterate through each phenotype file and extract the corresponding fitness values\n",
    "for file in pheno_files:    \n",
    "    df_pheno = pd.read_csv(file, sep=\"\\t\")  # Load phenotype data\n",
    "    data_fitness = df_pheno.iloc[shuffled_indices, 1].to_numpy()  # Shuffle phenotype data to match genotype order\n",
    "    fitness_list.append(data_fitness)  # Store fitness data for this environment\n",
    "\n",
    "# Select only the independent loci from genotype data and transform values from [0,1] to [-1,+1]\n",
    "geno_data = 2.0 * geno_data[:, sorted(ind_loci_list)] - 1.0\n",
    "\n",
    "# Convert the list of fitness arrays into a NumPy array and transpose for proper shape\n",
    "fitness = np.array(fitness_list)\n",
    "fitness = fitness.T  # Final shape is (99950, 18), where rows = samples, columns = environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be760bb-0cf6-4d3a-abbe-e545f246acbc",
   "metadata": {},
   "source": [
    "### Attention layer class in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283e9e7f-484c-46f7-b26f-e2ff6ebcdaae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ThreeLayerAttention(nn.Module):\n",
    "    def __init__(self, input_dim, query_dim, key_dim):\n",
    "        \"\"\"\n",
    "        Implements a three-layer attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimension of input features.\n",
    "            query_dim (int): Dimension of the query matrix.\n",
    "            key_dim (int): Dimension of the key matrix.\n",
    "        \"\"\"\n",
    "        super(ThreeLayerAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.key_dim = key_dim\n",
    "        self.seq_length = seq_length  \n",
    "\n",
    "        # Learnable weight matrices for the first attention layer\n",
    "        self.query_matrix_1 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_1 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_1 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "\n",
    "        # Learnable weight matrices for the second attention layer\n",
    "        self.query_matrix_2 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_2 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_2 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "        \n",
    "        # Learnable weight matrices for the third attention layer\n",
    "        self.query_matrix_3 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_3 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_3 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "        \n",
    "        # Learnable random projection matrix (reduces input dimensionality)\n",
    "        self.random_matrix = nn.Parameter(torch.empty(num_loci, low_dim))  \n",
    "\n",
    "        # Learnable coefficients for attended values\n",
    "        self.coeffs_attended = nn.Parameter(torch.empty(seq_length, input_dim))\n",
    "\n",
    "        # Learnable scalar offset for output adjustment\n",
    "        self.offset = nn.Parameter(torch.randn(1))\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"Initializes model parameters using a normal distribution with a small standard deviation.\"\"\"\n",
    "        init_scale = 0.03  # Small scale for initialization to prevent exploding gradients\n",
    "\n",
    "        for param in [self.query_matrix_1, self.key_matrix_1, self.value_matrix_1,\n",
    "                      self.query_matrix_2, self.key_matrix_2, self.value_matrix_2,\n",
    "                      self.query_matrix_3, self.key_matrix_3, self.value_matrix_3,\n",
    "                      self.random_matrix, self.coeffs_attended, self.offset]:\n",
    "            init.normal_(param, std=init_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through three layers of self-attention.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, num_loci, input_dim)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Final attended output of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        # Apply a random projection and concatenate it with the last feature, which consists entirely of ones\n",
    "        y = torch.cat((torch.matmul(x[:, :, :seq_length], self.random_matrix), x[:, :, -1:]), dim=2)\n",
    "\n",
    "        # First self-attention layer\n",
    "        query_1 = torch.matmul(y, self.query_matrix_1)\n",
    "        key_1 = torch.matmul(y, self.key_matrix_1)\n",
    "        value_1 = torch.matmul(y, self.value_matrix_1)\n",
    "        scores_1 = torch.matmul(query_1, key_1.transpose(1, 2))\n",
    "        scores_1 = torch.softmax(scores_1, dim=-1)  # Softmax for attention weighting\n",
    "        attended_values_1 = torch.matmul(scores_1, value_1)\n",
    "\n",
    "        # Second self-attention layer\n",
    "        query_2 = torch.matmul(attended_values_1, self.query_matrix_2)\n",
    "        key_2 = torch.matmul(attended_values_1, self.key_matrix_2)\n",
    "        value_2 = torch.matmul(attended_values_1, self.value_matrix_2)\n",
    "        scores_2 = torch.matmul(query_2, key_2.transpose(1, 2))\n",
    "        scores_2 = torch.softmax(scores_2, dim=-1)\n",
    "        attended_values_2 = torch.matmul(scores_2, value_2)\n",
    "\n",
    "        # Third self-attention layer\n",
    "        query_3 = torch.matmul(attended_values_2, self.query_matrix_3)\n",
    "        key_3 = torch.matmul(attended_values_2, self.key_matrix_3)\n",
    "        value_3 = torch.matmul(attended_values_2, self.value_matrix_3)\n",
    "        scores_3 = torch.matmul(query_3, key_3.transpose(1, 2))\n",
    "        scores_3 = torch.softmax(scores_3, dim=-1)\n",
    "        attended_values_3 = torch.matmul(scores_3, value_3)\n",
    "\n",
    "        # Compute final weighted sum using learned coefficients\n",
    "        attended_values_3 = torch.einsum(\"bij,ij->b\", attended_values_3, self.coeffs_attended)\n",
    "\n",
    "        # Add offset term to adjust output scale\n",
    "        output = attended_values_3 + self.offset\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a1d2a1-d463-4608-adc6-a8e611d6a15a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training each environment individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923453bf-1f6a-423a-9b30-caa7c06c7ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_to_file(filename, a, b):\n",
    "    \"\"\"\n",
    "    Appends validation R² score for each epoch to a file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the output file.\n",
    "        a (int): Epoch number.\n",
    "        b (float): R² score on validation data.\n",
    "    \"\"\"\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(f\"{a} {b}\\n\")\n",
    "\n",
    "# Set computation device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Split dataset into training (85%) and testing (15%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(geno_data, fitness, test_size=0.15, random_state=42)\n",
    "\n",
    "# Further split training set into training (85%) and validation (15%) sets\n",
    "X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "# Normalize target (fitness) data by computing mean and standard deviation from y_train2\n",
    "mean_values = np.nanmean(y_train2, axis=0)\n",
    "std_values = np.nanstd(y_train2, axis=0)\n",
    "\n",
    "# Standardize the target data\n",
    "y_train2 = (y_train2 - mean_values) / std_values\n",
    "y_val = (y_val - mean_values) / std_values\n",
    "y_test = (y_test - mean_values) / std_values\n",
    "\n",
    "# Define dimensions for the attention model\n",
    "low_dim = 12\n",
    "num_loci = len(ind_loci_list)\n",
    "seq_length = num_loci\n",
    "input_dim = low_dim + 1\n",
    "query_dim = low_dim + 1\n",
    "key_dim = low_dim + 1\n",
    "\n",
    "# Convert training genotype data to a PyTorch tensor\n",
    "X_train_tens = torch.tensor(X_train2).float()\n",
    "\n",
    "# Iterate over 18 environments for separate training runs\n",
    "for env in range(0, 18):\n",
    "    \n",
    "    # Extract filename and sheet name from phenotype file\n",
    "    file = pheno_files[env]\n",
    "    sheet = file.split('_')[2].split('.')[0]\n",
    "    \n",
    "    # Define save directory for model checkpoints\n",
    "    save_dir = f\"./single_env_attention_QTL_yeast_data/{sheet}\"\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Define path for validation R² scores and remove existing file\n",
    "    filename = f\"{save_dir}/validation_r2.txt\"\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "\n",
    "    # Initialize attention model and move it to GPU if available\n",
    "    attention_layer = ThreeLayerAttention(input_dim, query_dim, key_dim).to(device)\n",
    "\n",
    "    # Extract target data for the current environment\n",
    "    y_train2_env = y_train2[:, env]\n",
    "    y_val_env = y_val[:, env]\n",
    "    y_test_env = y_test[:, env]\n",
    "\n",
    "    # Convert target data to a PyTorch tensor\n",
    "    y_train_tens = torch.tensor(np.array(y_train2_env)).float()\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(attention_layer.parameters(), lr=0.001)\n",
    "\n",
    "    # Define batch size and chunking parameters\n",
    "    batch_size = 64\n",
    "    chunk_size = 50\n",
    "    num_epochs = 200\n",
    "    num_batches = X_train_tens.size(0) // batch_size\n",
    "\n",
    "    # TRAINING LOOP\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Shuffle training data before each epoch\n",
    "        indices = torch.randperm(X_train_tens.size(0))\n",
    "        train_input_shuffled = X_train_tens[indices]\n",
    "        train_target_shuffled = y_train_tens[indices]\n",
    "\n",
    "        # Mini-batch training using stochastic gradient descent\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "\n",
    "            # Extract mini-batch\n",
    "            mini_batch_input = train_input_shuffled[start_idx:end_idx]\n",
    "            mini_batch_target = train_target_shuffled[start_idx:end_idx]  \n",
    "\n",
    "            # Identify NaN values in target data    \n",
    "            nan_mask = np.isnan(mini_batch_target)\n",
    "            nan_indices = np.where(nan_mask)[0]\n",
    "\n",
    "            # Remove NaN entries from mini-batch\n",
    "            mini_batch_input = np.delete(mini_batch_input, nan_indices, axis=0).to(device)\n",
    "            mini_batch_target = np.delete(mini_batch_target, nan_indices).to(device)\n",
    "\n",
    "            # Create one-hot vector embedding for genotype data\n",
    "            one_hot_mini_batch_input = torch.zeros((mini_batch_input.shape[0], num_loci, num_loci), device=device)\n",
    "            indices = torch.arange(num_loci, device=device)\n",
    "            one_hot_mini_batch_input[:, indices, indices] = mini_batch_input.squeeze()\n",
    "            one_hot_mini_batch_input = torch.cat((one_hot_mini_batch_input, torch.ones((mini_batch_input.shape[0], seq_length, 1)).to(device)), dim=2)\n",
    "\n",
    "            # Forward pass\n",
    "            train_output = attention_layer(one_hot_mini_batch_input).to(device)\n",
    "\n",
    "            # Compute loss\n",
    "            train_loss = loss_function(train_output, mini_batch_target)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Save model state after each epoch\n",
    "        model_state_path = os.path.join(save_dir, f\"epoch_{epoch}.pt\")\n",
    "        torch.save(attention_layer.state_dict(), model_state_path)\n",
    "\n",
    "        # VALIDATION STEP\n",
    "        y_pred = torch.tensor([]).to(device)\n",
    "\n",
    "        # Identify NaN values in validation fitness data\n",
    "        nan_mask = np.isnan(y_val_env)\n",
    "        nan_indices = np.where(nan_mask)[0]\n",
    "\n",
    "        # Remove NaN values from validation data\n",
    "        X_val_env = np.delete(X_val, nan_indices, axis=0)\n",
    "        y_val_env2 = np.delete(y_val_env, nan_indices, axis=0)\n",
    "\n",
    "        # Convert validation genotype data to PyTorch tensor\n",
    "        X_val_tens = torch.tensor(np.array(X_val_env)).float().to(device)\n",
    "\n",
    "        # Avoid memory issues by processing validation data in chunks\n",
    "        for i in range(0, len(X_val_tens), chunk_size):\n",
    "            chunk = X_val_tens[i:i + chunk_size].to(device)\n",
    "\n",
    "            if i + chunk_size > len(X_val_tens):\n",
    "                chunk_size_actual = len(X_val_tens) - i\n",
    "            else:\n",
    "                chunk_size_actual = chunk_size\n",
    "\n",
    "            # Create one-hot vector embedding for validation data\n",
    "            one_hot_val_input = torch.zeros((chunk_size_actual, num_loci, num_loci + 18), device=device)\n",
    "            indices = torch.arange(num_loci, device=device)\n",
    "            one_hot_val_input[:, indices, indices] = chunk.squeeze(dim=1)\n",
    "            one_hot_val_input = torch.cat((one_hot_val_input, torch.ones((chunk_size_actual, seq_length, 1)).to(device)), dim=2)\n",
    "\n",
    "            # Compute predictions\n",
    "            with torch.no_grad():\n",
    "                i_pred = attention_layer(one_hot_val_input)\n",
    "\n",
    "            y_pred = torch.cat((y_pred, i_pred), dim=0)\n",
    "\n",
    "        # Compute R² score for validation data\n",
    "        val_r_squared = r2_score(y_val_env2, y_pred.cpu())\n",
    "\n",
    "        # Save validation R² score for current epoch\n",
    "        write_to_file(filename, epoch, val_r_squared)\n",
    "\n",
    "        # Clear GPU cache\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8759e-42cf-4054-9e43-8528b7d9076a",
   "metadata": {},
   "source": [
    "### Model prediction performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e25cc-83cb-4b98-8161-fce61e730bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(filename, a, b):\n",
    "    \"\"\"\n",
    "    Appends test R² score for each environment to a file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the output file.\n",
    "        a (str): Environment name.\n",
    "        b (float): R² score on test data.\n",
    "    \"\"\"\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(f\"{a} {b}\\n\")\n",
    "\n",
    "# Define the file path for saving test R² scores\n",
    "filename2 = \"./single_env_attention_QTL_yeast_data/test_r2_scores.txt\"\n",
    "\n",
    "# Remove existing test R² score file if it exists\n",
    "if os.path.exists(filename2):\n",
    "    os.remove(filename2)\n",
    "\n",
    "# Create directories for saving test predictions and true values\n",
    "if not os.path.exists(\"./single_env_attention_QTL_yeast_data/test_predictions\"):\n",
    "    os.makedirs(\"./single_env_attention_QTL_yeast_data/test_predictions\")\n",
    "\n",
    "if not os.path.exists(\"./single_env_attention_QTL_yeast_data/test_true\"):\n",
    "    os.makedirs(\"./single_env_attention_QTL_yeast_data/test_true\")\n",
    "\n",
    "# Iterate over all 18 environments\n",
    "for env in range(18):\n",
    "    \n",
    "    file = pheno_files[env]\n",
    "    sheet = file.split('_')[2].split('.')[0]  # Extract environment name\n",
    "\n",
    "    # Retrieve the epoch with the highest R² score from validation set\n",
    "    filename = f\"./single_env_attention_QTL_yeast_data/{sheet}/validation_r2.txt\"\n",
    "    data = pd.read_csv(filename, sep='\\s+', header=None)\n",
    "    max_row_index = data[1].idxmax()  # Find the index of the maximum R² score\n",
    "    max_row = data.loc[max_row_index]  # Get the corresponding row\n",
    "    max_first_column_value = max_row[0]  # Extract the epoch number with the best performance\n",
    "\n",
    "    # Set computation device (GPU if available)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Split dataset into training (85%) and testing (15%) sets for the current environment\n",
    "    X_train, X_test, y_train, y_test = train_test_split(geno_data, fitness[:, env], test_size=0.15, random_state=42)\n",
    "    X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "    # Compute mean and standard deviation from y_train2 for normalization\n",
    "    mean_values = np.nanmean(y_train2, axis=0)\n",
    "    std_values = np.nanstd(y_train2, axis=0)\n",
    "\n",
    "    # Normalize target fitness data\n",
    "    y_train2 = (y_train2 - mean_values) / std_values\n",
    "    y_val = (y_val - mean_values) / std_values\n",
    "    y_test = (y_test - mean_values) / std_values\n",
    "\n",
    "    # Define model input dimensions\n",
    "    low_dim = 12\n",
    "    num_loci = len(ind_loci_list)\n",
    "    seq_length = num_loci\n",
    "    input_dim = low_dim + 1\n",
    "    query_dim = low_dim + 1\n",
    "    key_dim = low_dim + 1\n",
    "    chunk_size = 50\n",
    "\n",
    "    # Initialize the attention model and move it to the selected device\n",
    "    attention_layer = ThreeLayerAttention(input_dim, query_dim, key_dim).to(device)\n",
    "\n",
    "    # Load the trained model checkpoint with the best validation performance\n",
    "    epoch = int(max_first_column_value)\n",
    "    model_path = f\"./single_env_attention_QTL_yeast_data/{sheet}/epoch_{epoch}.pt\"\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    attention_layer.load_state_dict(state_dict)\n",
    "    attention_layer.to(device)\n",
    "    attention_layer.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Identify NaN values in test fitness data\n",
    "    nan_mask = np.isnan(y_test)\n",
    "    nan_indices = np.where(nan_mask)[0]\n",
    "    y_pred_test_env = torch.tensor([]).to(device)\n",
    "\n",
    "    # Remove NaN values from test data\n",
    "    X_test_env = np.delete(X_test, nan_indices, axis=0)\n",
    "    y_test_env = np.delete(y_test, nan_indices, axis=0)\n",
    "\n",
    "    # Convert test genotype data to PyTorch tensor\n",
    "    X_test_tens = torch.tensor(np.array(X_test_env)).float().to(device)\n",
    "\n",
    "    # PROCESS TEST DATA IN CHUNKS TO AVOID MEMORY ISSUES\n",
    "    for i in range(0, len(X_test_tens), chunk_size):\n",
    "        chunk = X_test_tens[i:i + chunk_size].to(device)\n",
    "\n",
    "        if i + chunk_size > len(X_test_tens):\n",
    "            chunk_size_actual = len(X_test_tens) - i\n",
    "        else:\n",
    "            chunk_size_actual = chunk_size\n",
    "\n",
    "        # Create one-hot vector encoding for test data\n",
    "        one_hot_test_input = torch.zeros((chunk_size_actual, num_loci, num_loci + 18), device=device)\n",
    "        indices = torch.arange(num_loci, device=device)\n",
    "        one_hot_test_input[:, indices, indices] = chunk.squeeze(dim=1)\n",
    "        one_hot_test_input = torch.cat((one_hot_test_input, torch.ones((chunk_size_actual, seq_length, 1)).to(device)), dim=2)\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            i_pred = attention_layer(one_hot_test_input)\n",
    "\n",
    "        y_pred_test_env = torch.cat((y_pred_test_env, i_pred), dim=0)\n",
    "\n",
    "    # Compute R² score for test set\n",
    "    test_r2 = r2_score(y_test_env, y_pred_test_env.cpu())\n",
    "    write_to_file(filename2, sheet, test_r2)\n",
    "    print(sheet, test_r2)\n",
    "\n",
    "    # Unnormalize the predictions to the original scale\n",
    "    y_test_env_unnorm = y_test_env * std_values + mean_values\n",
    "    y_pred_test_env_unnorm = y_pred_test_env.cpu().numpy() * std_values + mean_values\n",
    "\n",
    "    # Save unnormalized predictions and true values\n",
    "    np.save(f\"./single_env_attention_QTL_yeast_data/test_predictions/{sheet}.npy\", y_pred_test_env_unnorm)\n",
    "    np.save(f\"./single_env_attention_QTL_yeast_data/test_true/{sheet}.npy\", y_test_env_unnorm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
