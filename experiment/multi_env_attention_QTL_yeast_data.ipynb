{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7189319-9f83-4f11-b913-81bda0a81bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pickle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbe7d6-9068-4838-a747-9f657f24e481",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d027bd5d-8171-44d2-b97b-f79ede71b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load genotype data from a NumPy file\n",
    "geno_data = np.load('merged_geno_data.npy')\n",
    "\n",
    "# List of phenotype (fitness) data files corresponding to 18 different environments\n",
    "pheno_files = ['pheno_data_23C.txt', 'pheno_data_25C.txt', 'pheno_data_27C.txt', 'pheno_data_30C.txt', \n",
    "               'pheno_data_33C.txt', 'pheno_data_35C.txt', 'pheno_data_37C.txt', 'pheno_data_cu.txt',\n",
    "               'pheno_data_suloc.txt', 'pheno_data_ynb.txt', 'pheno_data_eth.txt', 'pheno_data_gu.txt', \n",
    "               'pheno_data_li.txt', 'pheno_data_mann.txt', 'pheno_data_mol.txt', 'pheno_data_raff.txt', \n",
    "               'pheno_data_sds.txt', 'pheno_data_4NQO.txt']\n",
    "\n",
    "# Shuffle the genotype data to randomize sample order\n",
    "num_segregants = geno_data.shape[0]  # Total number of segregants (samples)\n",
    "shuffled_indices = list(range(num_segregants))  # Create index list for shuffling\n",
    "random.seed(0)  # Set a fixed seed for reproducibility\n",
    "random.shuffle(shuffled_indices)  # Shuffle the indices\n",
    "geno_data = geno_data[shuffled_indices]  # Apply shuffled indices to genotype data\n",
    "\n",
    "# Load the list of independent loci for feature selection\n",
    "ind_loci_list = np.load('ind_loci_list_3.npy')\n",
    "\n",
    "# Initialize a list to store fitness values across all 18 environments\n",
    "fitness_list = []\n",
    "\n",
    "# Iterate through each phenotype file and extract the corresponding fitness values\n",
    "for file in pheno_files:    \n",
    "    df_pheno = pd.read_csv(file, sep=\"\\t\")  # Load phenotype data\n",
    "    data_fitness = df_pheno.iloc[shuffled_indices, 1].to_numpy()  # Shuffle phenotype data to match genotype order\n",
    "    fitness_list.append(data_fitness)  # Store fitness data for this environment\n",
    "\n",
    "# Select only the independent loci from genotype data and transform values from [0,1] to [-1,+1]\n",
    "geno_data = 2.0 * geno_data[:, sorted(ind_loci_list)] - 1.0\n",
    "\n",
    "# Convert the list of fitness arrays into a NumPy array and transpose for proper shape\n",
    "fitness = np.array(fitness_list)\n",
    "fitness = fitness.T  # Final shape is (99950, 18), where rows = samples, columns = environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be760bb-0cf6-4d3a-abbe-e545f246acbc",
   "metadata": {},
   "source": [
    "### Attention layer class in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283e9e7f-484c-46f7-b26f-e2ff6ebcdaae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ThreeLayerAttention(nn.Module):\n",
    "    def __init__(self, input_dim, query_dim, key_dim, seq_length):\n",
    "        \"\"\"\n",
    "        Initializes a three-layer attention model.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimension of input features.\n",
    "            query_dim (int): Dimension of the query matrix.\n",
    "            key_dim (int): Dimension of the key matrix.\n",
    "            seq_length (int): Length of input sequence.\n",
    "        \"\"\"\n",
    "        super(ThreeLayerAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.key_dim = key_dim\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Learnable parameters for the first attention layer\n",
    "        self.query_matrix_1 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_1 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_1 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "\n",
    "        # Learnable parameters for the second attention layer\n",
    "        self.query_matrix_2 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_2 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_2 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "        \n",
    "        # Learnable parameters for the third attention layer\n",
    "        self.query_matrix_3 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_3 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_3 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "        \n",
    "        # Learnable random projection matrix for dimensionality reduction\n",
    "        self.random_matrix = nn.Parameter(torch.empty(num_loci, low_dim))\n",
    "\n",
    "        # Additional learnable coefficients for final transformation\n",
    "        self.coeffs_attended = nn.Parameter(torch.empty(seq_length, input_dim))\n",
    "        self.offset = nn.Parameter(torch.randn(1))  # Learnable bias term\n",
    "\n",
    "        # Initialize model parameters\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"Initializes model parameters with a small normal distribution.\"\"\"\n",
    "        init_scale = 0.03  # Standard deviation for parameter initialization\n",
    "\n",
    "        # Initialize all weight matrices and learnable parameters\n",
    "        for param in [self.query_matrix_1, self.key_matrix_1, self.value_matrix_1,\n",
    "                      self.query_matrix_2, self.key_matrix_2, self.value_matrix_2,\n",
    "                      self.query_matrix_3, self.key_matrix_3, self.value_matrix_3,\n",
    "                      self.random_matrix, self.coeffs_attended, self.offset]:\n",
    "            init.normal_(param, std=init_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the three-layer self-attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_length, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Final attended output of shape (batch_size,).\n",
    "        \"\"\"\n",
    "        # Apply random projection to the sequence data\n",
    "        y = torch.cat((torch.matmul(x[:, :seq_length-1, :seq_length-1], self.random_matrix), \n",
    "                       x[:, :-1, -19:]), dim=2)\n",
    "\n",
    "        # Concatenate the transformed sequence with the last element \n",
    "        x = torch.cat((y, x[:, -1, -low_dim-19:].unsqueeze(1)), dim=1)\n",
    "\n",
    "        # First self-attention layer\n",
    "        query_1 = torch.matmul(x, self.query_matrix_1)\n",
    "        key_1 = torch.matmul(x, self.key_matrix_1)\n",
    "        value_1 = torch.matmul(x, self.value_matrix_1)\n",
    "        scores_1 = torch.matmul(query_1, key_1.transpose(1, 2))  # Compute attention scores\n",
    "        scores_1 = torch.softmax(scores_1, dim=-1)  # Normalize scores with softmax\n",
    "        attended_values_1 = torch.matmul(scores_1, value_1)  # Apply attention weights\n",
    "\n",
    "        # Second self-attention layer\n",
    "        query_2 = torch.matmul(attended_values_1, self.query_matrix_2)\n",
    "        key_2 = torch.matmul(attended_values_1, self.key_matrix_2)\n",
    "        value_2 = torch.matmul(attended_values_1, self.value_matrix_2)\n",
    "        scores_2 = torch.matmul(query_2, key_2.transpose(1, 2))\n",
    "        scores_2 = torch.softmax(scores_2, dim=-1)\n",
    "        attended_values_2 = torch.matmul(scores_2, value_2)\n",
    "\n",
    "        # Third self-attention layer\n",
    "        query_3 = torch.matmul(attended_values_2, self.query_matrix_3)\n",
    "        key_3 = torch.matmul(attended_values_2, self.key_matrix_3)\n",
    "        value_3 = torch.matmul(attended_values_2, self.value_matrix_3)\n",
    "        scores_3 = torch.matmul(query_3, key_3.transpose(1, 2))\n",
    "        scores_3 = torch.softmax(scores_3, dim=-1)\n",
    "        attended_values_3 = torch.matmul(scores_3, value_3)\n",
    "\n",
    "        # Compute final weighted sum using learned coefficients\n",
    "        attended_values_3 = torch.einsum(\"bij,ij->b\", attended_values_3, self.coeffs_attended)\n",
    "\n",
    "        # Add offset term to adjust output scale\n",
    "        output = attended_values_3 + self.offset\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604cca67-7f64-4995-bf9e-f23e90e353c9",
   "metadata": {},
   "source": [
    "### Create one-hot environment embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd0ce45-0265-4da7-949e-35736629c8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def env_embed(nan_indices):\n",
    "    \"\"\"\n",
    "    Creates an environment embedding tensor with one-hot encoding for 18 environments.\n",
    "\n",
    "    Args:\n",
    "        nan_indices (list): Indices of data points that should be removed from the output.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A tensor of shape (filtered_batch_size, 1, num_loci + 18),\n",
    "                where the last 18 elements encode the environment identity.\n",
    "    \"\"\"\n",
    "    # Initialize a zero tensor with shape (batch_size, 1, num_loci + 18)\n",
    "    tensor = torch.zeros((batch_size, 1, num_loci + 18))\n",
    "\n",
    "    # Assign one-hot encoding for environments\n",
    "    for i in range(batch_size):\n",
    "        position = i // num_elements  # Determine the environment index\n",
    "        tensor[i, 0, num_loci + position] = 1  # Set the corresponding environment bit to 1\n",
    "\n",
    "    # Remove indices corresponding to NaN values\n",
    "    valid_indices = [i for i in range(batch_size) if i not in nan_indices]\n",
    "    tensor = tensor[valid_indices, :, :]  # Select only valid rows\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a1d2a1-d463-4608-adc6-a8e611d6a15a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training all the environments combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d5178-7794-4c95-b147-2e67eec7685c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define dimensionality of the low-dimensional space\n",
    "low_dim = 12\n",
    "\n",
    "# Directory for saving model parameters and results\n",
    "save_dir = \"./multi_env_attention_QTL_yeast_data/\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# File for storing R² validation scores\n",
    "filename = \"./multi_env_attention_QTL_yeast_data/val_r2.txt\"\n",
    "\n",
    "def write_to_file(filename, a, b, c):\n",
    "    \"\"\"\n",
    "    Writes validation R² scores to a file after each epoch.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the output file.\n",
    "        a (int): Low-dimensional embedding size.\n",
    "        b (int): Epoch number.\n",
    "        c (float): R² score on validation data.\n",
    "    \"\"\"\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(f\"{a} {b} {c}\\n\")\n",
    "\n",
    "# Move model and tensors to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Split data into training (85%) and testing (15%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(geno_data, fitness, test_size=0.15, random_state=42)\n",
    "\n",
    "# Further split training set into training (85%) and validation (15%) sets\n",
    "X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "# Compute mean and standard deviation for normalization\n",
    "mean_values = np.nanmean(y_train2, axis=0)\n",
    "std_values = np.nanstd(y_train2, axis=0)\n",
    "\n",
    "# Normalize target (fitness) data\n",
    "y_train2 = (y_train2 - mean_values) / std_values\n",
    "y_val = (y_val - mean_values) / std_values\n",
    "y_test = (y_test - mean_values) / std_values\n",
    "\n",
    "# Convert genotype and fitness data to PyTorch tensors\n",
    "X_train_tens = torch.tensor(X_train2).float()\n",
    "y_train_tens = torch.tensor(np.array(y_train2)).float()\n",
    "\n",
    "# Define model input dimensions\n",
    "num_loci = len(ind_loci_list)\n",
    "seq_length = num_loci + 1\n",
    "input_dim = low_dim + 18 + 1\n",
    "query_dim = low_dim + 18 + 1\n",
    "key_dim = low_dim + 18 + 1\n",
    "\n",
    "# Initialize the attention model and move it to device\n",
    "attention_layer = ThreeLayerAttention(input_dim, query_dim, key_dim, seq_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(attention_layer.parameters(), lr=0.001)\n",
    "\n",
    "# Define batch and training parameters\n",
    "num_elements = 4  # Number of segregants per environment\n",
    "batch_size = 18 * num_elements\n",
    "chunk_size = 50\n",
    "num_epochs = 3000\n",
    "num_batches = X_train_tens.size(0) // batch_size\n",
    "\n",
    "# TRAINING LOOP\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Shuffle training data before each epoch\n",
    "    indices = torch.randperm(X_train_tens.size(0))\n",
    "    train_input_shuffled = X_train_tens[indices]\n",
    "    train_target_shuffled = y_train_tens[indices].T\n",
    "\n",
    "    # Mini-batch training using stochastic gradient descent\n",
    "    for i in range(num_batches):\n",
    "        \n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "                \n",
    "        fitness_18_env = []\n",
    "\n",
    "        # Collect num_elements fitness values per environment\n",
    "        for i, row in enumerate(train_target_shuffled):\n",
    "            start_col = start_idx + i * num_elements\n",
    "            end_col = start_col + num_elements\n",
    "            fitness_18_env.extend(row[start_col:end_col])\n",
    "\n",
    "        # Create mini-batches\n",
    "        mini_batch_input = train_input_shuffled[start_idx:end_idx]\n",
    "        mini_batch_target = torch.tensor(fitness_18_env)\n",
    "\n",
    "        # Remove NaN values from target\n",
    "        nan_mask = np.isnan(mini_batch_target)\n",
    "        nan_indices = np.where(nan_mask)[0]\n",
    "        mini_batch_input = np.delete(mini_batch_input, nan_indices, axis=0).to(device)\n",
    "        mini_batch_target = np.delete(mini_batch_target, nan_indices).to(device)\n",
    "\n",
    "        # Create environment embeddings\n",
    "        mini_batch_env = torch.tensor(np.delete(np.repeat(np.arange(18), 4), nan_indices)).float().to(device)\n",
    "        mini_batch_input_env = torch.cat((mini_batch_input, mini_batch_env.view(-1, 1)), dim=1)\n",
    "\n",
    "        # Create one-hot vector embedding for loci\n",
    "        one_hot_mini_batch_input = torch.zeros((mini_batch_input.shape[0], num_loci, num_loci + 18), device=device)\n",
    "        indices = torch.arange(num_loci, device=device)\n",
    "        one_hot_mini_batch_input[:, indices, indices] = mini_batch_input.squeeze()\n",
    "        \n",
    "        # Generate environment embeddings and concatenate with input\n",
    "        env_emb = env_embed(nan_indices).float().to(device)\n",
    "        one_hot_mini_batch_input = torch.cat((one_hot_mini_batch_input, env_emb), dim=1)\n",
    "        one_hot_mini_batch_input = torch.cat((one_hot_mini_batch_input, torch.ones((mini_batch_input.shape[0], seq_length, 1)).to(device)), dim=2)\n",
    "\n",
    "        # Forward pass\n",
    "        train_output = attention_layer(one_hot_mini_batch_input).to(device)\n",
    "        train_loss = loss_function(train_output, mini_batch_target)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save model after each epoch\n",
    "    model_state_path = os.path.join(save_dir, f\"epoch_{epoch}.pt\")\n",
    "    torch.save(attention_layer.state_dict(), model_state_path)\n",
    "\n",
    "    # VALIDATION STEP\n",
    "    y_pred = torch.tensor([]).to(device)\n",
    "    y_val_all = np.array([])\n",
    "\n",
    "    for env in range(18):\n",
    "        env_emb = np.zeros(num_loci + 18)\n",
    "        env_emb[int(num_loci + env)] = 1\n",
    "        env_emb = torch.tensor(env_emb).float().to(device)\n",
    "\n",
    "        y_val_env = y_val.T[env]\n",
    "\n",
    "        # Remove NaN values from validation fitness data\n",
    "        nan_mask = np.isnan(y_val_env)\n",
    "        nan_indices = np.where(nan_mask)[0]\n",
    "        X_val_env = np.delete(X_val, nan_indices, axis=0)\n",
    "        y_val_env = np.delete(y_val_env, nan_indices, axis=0)\n",
    "\n",
    "        # Convert validation data to PyTorch tensor\n",
    "        X_val_tens = torch.tensor(np.array(X_val_env)).float().to(device)\n",
    "\n",
    "        # Process validation data in chunks\n",
    "        for i in range(0, len(X_val_tens), chunk_size):\n",
    "            chunk = X_val_tens[i:i + chunk_size].to(device)\n",
    "            chunk_size_actual = chunk_size if i + chunk_size <= len(X_val_tens) else len(X_val_tens) - i\n",
    "\n",
    "            # Create environment embeddings for validation chunk\n",
    "            chunk_env = torch.tensor(np.repeat(env, chunk_size_actual)).float().to(device)\n",
    "            chunk_input_env = torch.cat((chunk, chunk_env.view(-1, 1)), dim=1)\n",
    "\n",
    "            one_hot_val_input = torch.zeros((chunk_size_actual, num_loci, num_loci + 18), device=device)\n",
    "            indices = torch.arange(num_loci, device=device)\n",
    "            one_hot_val_input[:, indices, indices] = chunk.squeeze(dim=1)\n",
    "            env_emb2 = env_emb.unsqueeze(0).unsqueeze(1).repeat(one_hot_val_input.shape[0], 1, 1)\n",
    "            one_hot_val_input = torch.cat((one_hot_val_input, env_emb2), dim=1)\n",
    "            one_hot_val_input = torch.cat((one_hot_val_input, torch.ones((chunk_size_actual, seq_length, 1)).to(device)), dim=2)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                i_pred = attention_layer(one_hot_val_input)\n",
    "\n",
    "            y_pred = torch.cat((y_pred, i_pred), dim=0)\n",
    "\n",
    "        y_val_all = np.concatenate((y_val_all, y_val_env))\n",
    "\n",
    "    val_r_squared = r2_score(y_val_all, y_pred.cpu())\n",
    "\n",
    "    # Save validation performance\n",
    "    write_to_file(filename, low_dim, epoch, val_r_squared)\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f50bba-32b6-48f7-9fd5-2f7ac739cf1e",
   "metadata": {},
   "source": [
    "### Model prediction performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5dc8bc-a35c-4d1a-98cd-018a57ea617c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define low-dimensional space size\n",
    "low_dim = 12\n",
    "\n",
    "# Load validation R² scores and retrieve the best epoch\n",
    "filename = \"./multi_env_attention_QTL_yeast_data/val_r2.txt\"\n",
    "data = pd.read_csv(filename, sep='\\s+', header=None)\n",
    "max_row_index = data[2].idxmax()  # Index of max validation R² score\n",
    "max_row = data.loc[max_row_index]  # Retrieve the corresponding row\n",
    "max_second_column_value = max_row[1]  # Get epoch with highest R² score\n",
    "\n",
    "# Move model and tensors to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Split dataset into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(geno_data, fitness, test_size=0.15, random_state=42)\n",
    "X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "# Compute mean and standard deviation for normalization\n",
    "mean_values = np.nanmean(y_train2, axis=0)\n",
    "std_values = np.nanstd(y_train2, axis=0)\n",
    "\n",
    "# Normalize target (fitness) data\n",
    "y_train2 = (y_train2 - mean_values) / std_values\n",
    "y_val = (y_val - mean_values) / std_values\n",
    "y_test = (y_test - mean_values) / std_values\n",
    "\n",
    "# Define model input dimensions\n",
    "chunk_size = 100\n",
    "num_loci = len(ind_loci_list)\n",
    "seq_length = num_loci + 1\n",
    "input_dim = low_dim + 18 + 1\n",
    "query_dim = low_dim + 18 + 1\n",
    "key_dim = low_dim + 18 + 1\n",
    "\n",
    "# Initialize the ThreeLayerAttention model and move it to device\n",
    "attention_layer = ThreeLayerAttention(input_dim, query_dim, key_dim, seq_length).to(device)\n",
    "\n",
    "# Load the best-performing model checkpoint\n",
    "epoch = int(max_second_column_value)\n",
    "model_path = f\"./multi_env_attention_QTL_yeast_data/epoch_{epoch}.pt\"\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "attention_layer.load_state_dict(state_dict)\n",
    "attention_layer.to(device)  # Move model to the appropriate device\n",
    "attention_layer.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Lists to store results for later R² calculations\n",
    "y_test_all = []\n",
    "y_pred_test_all = []\n",
    "\n",
    "# File to store test R² scores\n",
    "filename2 = \"./multi_env_attention_QTL_yeast_data/test_r2_scores.txt\"\n",
    "\n",
    "# Remove existing test R² score file if it exists\n",
    "if os.path.exists(filename2):\n",
    "    os.remove(filename2)\n",
    "\n",
    "# Directory for saving test predictions\n",
    "save_dir = \"./multi_env_attention_QTL_yeast_data/predictions_test\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "def write_to_file(filename, a, b):\n",
    "    \"\"\"\n",
    "    Writes test R² scores to a file after evaluation.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the output file.\n",
    "        a (str): Environment name.\n",
    "        b (float): R² score on test data.\n",
    "    \"\"\"\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(f\"{a} {b}\\n\")\n",
    "\n",
    "# Iterate over 18 environments and evaluate test performance\n",
    "for env in range(18):\n",
    "    file = pheno_files[env]\n",
    "    sheet = file.split('_')[2].split('.')[0]  # Extract environment name\n",
    "\n",
    "    # Create one-hot encoding for the current environment\n",
    "    env_emb = np.zeros(num_loci + 18)\n",
    "    env_emb[num_loci + env] = 1\n",
    "    env_emb = torch.tensor(env_emb).float().to(device)\n",
    "\n",
    "    # Extract test fitness data for the current environment\n",
    "    y_test_env = y_test.T[env]\n",
    "\n",
    "    # Identify and remove NaN values\n",
    "    nan_mask = np.isnan(y_test_env)\n",
    "    nan_indices = np.where(nan_mask)[0]\n",
    "    y_pred_test_env = torch.tensor([]).to(device)\n",
    "\n",
    "    X_test_env = np.delete(X_test, nan_indices, axis=0)\n",
    "    y_test_env = np.delete(y_test_env, nan_indices, axis=0)\n",
    "\n",
    "    # Convert test data to PyTorch tensors\n",
    "    X_test_tens = torch.tensor(np.array(X_test_env)).float().to(device)\n",
    "\n",
    "    # Process test data in chunks to prevent memory issues\n",
    "    for i in range(0, len(X_test_tens), chunk_size):\n",
    "        chunk = X_test_tens[i:i + chunk_size].to(device)\n",
    "\n",
    "        if i + chunk_size > len(X_test_tens):\n",
    "            chunk_size_actual = len(X_test_tens) - i\n",
    "        else:\n",
    "            chunk_size_actual = chunk_size\n",
    "\n",
    "        # Create environment embedding for the chunk\n",
    "        chunk_env = torch.tensor(np.repeat(env, chunk_size_actual)).float().to(device)\n",
    "        chunk_input_env = torch.cat((chunk, chunk_env.view(-1, 1)), dim=1)\n",
    "\n",
    "        # Create one-hot vector encoding for test data\n",
    "        one_hot_test_input = torch.zeros((chunk_size_actual, num_loci, num_loci + 18), device=device)\n",
    "        indices = torch.arange(num_loci, device=device)\n",
    "        one_hot_test_input[:, indices, indices] = chunk.squeeze(dim=1)\n",
    "\n",
    "        # Concatenate environment embedding\n",
    "        env_emb2 = env_emb.unsqueeze(0).unsqueeze(1).repeat(one_hot_test_input.shape[0], 1, 1)\n",
    "        one_hot_test_input = torch.cat((one_hot_test_input, env_emb2), dim=1)\n",
    "        one_hot_test_input = torch.cat((one_hot_test_input, torch.ones((chunk_size_actual, seq_length, 1)).to(device)), dim=2)\n",
    "\n",
    "        # Perform inference with the model\n",
    "        with torch.no_grad():\n",
    "            i_pred = attention_layer(one_hot_test_input)\n",
    "\n",
    "        # Store predictions\n",
    "        y_pred_test_env = torch.cat((y_pred_test_env, i_pred), dim=0)\n",
    "\n",
    "    # Unnormalize predictions\n",
    "    y_test_env_unnorm = y_test_env * std_values[env] + mean_values[env]\n",
    "    y_pred_test_env_unnorm = y_pred_test_env.cpu().numpy() * std_values[env] + mean_values[env]\n",
    "\n",
    "    # Store predictions for overall R² calculation\n",
    "    y_test_all.extend(y_test_env)\n",
    "    y_pred_test_all.extend(y_pred_test_env.cpu().numpy())\n",
    "\n",
    "    # Save predictions for the current environment\n",
    "    np.save(f\"{save_dir}/y_test_{sheet}.npy\", y_test_env_unnorm)\n",
    "    np.save(f\"{save_dir}/y_pred_test_{sheet}.npy\", y_pred_test_env_unnorm)\n",
    "\n",
    "    # Compute and save R² score for the current environment\n",
    "    test_r2 = r2_score(y_test_env, y_pred_test_env.cpu())\n",
    "    write_to_file(filename2, sheet, test_r2)\n",
    "    print(sheet, test_r2)\n",
    "\n",
    "# Calculate and print overall R² score\n",
    "overall_r2 = r2_score(y_test_all, y_pred_test_all)\n",
    "print(\"Overall R²:\", overall_r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
