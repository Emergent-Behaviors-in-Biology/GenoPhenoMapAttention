{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7189319-9f83-4f11-b913-81bda0a81bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11aa7871-3317-4371-bd3f-1d5e3969fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the genotype data\n",
    "geno_data = np.load(\"merged_geno_data.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf7045-4766-4b52-8436-87b335b47364",
   "metadata": {},
   "source": [
    "### Pick loci from the list of independent loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610f1c33-da2a-4816-8d39-147d6b1c6a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define temperature conditions\n",
    "temps = np.array([30.0])\n",
    "\n",
    "def pick_loci(num_loci):\n",
    "    \"\"\"\n",
    "    Selects a subset of loci that meet a threshold based on mean allele frequency.\n",
    "\n",
    "    Args:\n",
    "        num_loci (int): The number of loci to select.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: A genotype matrix of shape (num_segregants, num_loci) \n",
    "                 with values mapped from [0,1] to [-1,+1].\n",
    "    \"\"\"\n",
    "    # Get the number of segregants (samples)\n",
    "    num_segregants = geno_data.shape[0]\n",
    "\n",
    "    # Shuffle segregant indices for randomization\n",
    "    shuffled_indices = list(range(num_segregants))\n",
    "    random.seed(0)  # Ensure reproducibility\n",
    "    random.shuffle(shuffled_indices)\n",
    "\n",
    "    # Apply shuffling to genotype data\n",
    "    geno_data2 = geno_data[shuffled_indices]\n",
    "\n",
    "    # Load independent loci list\n",
    "    loci_list = np.load(\"ind_loci_list_3.npy\")\n",
    "    loci_list = np.sort(loci_list)  # Sort loci indices in ascending order\n",
    "    loci_list_reduced = []  # List to store selected loci\n",
    "\n",
    "    # Iterate through loci and filter based on mean allele frequency\n",
    "    for i in loci_list:\n",
    "        avg_loci = (2.0 * geno_data2[:, i] - 1.0).mean()  # Convert allele states to [-1, 1] and compute mean\n",
    "        \n",
    "        if abs(avg_loci) < 0.05:  # Select loci with mean close to zero (balanced representation)\n",
    "            loci_list_reduced.append(i)\n",
    "        \n",
    "        if len(loci_list_reduced) > num_loci - 1:  # Stop when required number of loci is reached\n",
    "            break\n",
    "\n",
    "    # Extract the genotype data for selected loci and map values from [0,1] to [-1,+1]\n",
    "    genotype = 2.0 * geno_data2[:, np.array(loci_list_reduced)] - 1.0\n",
    "    \n",
    "    return genotype\n",
    "\n",
    "# Define number of loci to be selected\n",
    "num_loci = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be760bb-0cf6-4d3a-abbe-e545f246acbc",
   "metadata": {},
   "source": [
    "### Attention layer class in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6bcec0-fe17-4fae-bffa-ec22978b2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerAttention(nn.Module):\n",
    "    def __init__(self, input_dim, query_dim, key_dim, seq_length):\n",
    "        \"\"\"\n",
    "        Implements a three-layer self-attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimension of the input features.\n",
    "            query_dim (int): Dimension of the query matrix.\n",
    "            key_dim (int): Dimension of the key matrix.\n",
    "            seq_length (int): Length of the input sequence.\n",
    "        \"\"\"\n",
    "        super(ThreeLayerAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.key_dim = key_dim\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Learnable weight matrices for the first attention layer\n",
    "        self.query_matrix_1 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_1 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_1 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "\n",
    "        # Learnable weight matrices for the second attention layer\n",
    "        self.query_matrix_2 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_2 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_2 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "        \n",
    "        # Learnable weight matrices for the third attention layer\n",
    "        self.query_matrix_3 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_3 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_3 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "        \n",
    "        # Learnable random projection matrix (reduces input dimensionality)\n",
    "        self.random_matrix = nn.Parameter(torch.empty(seq_length, low_dim))\n",
    "\n",
    "        # Learnable coefficients for attended values\n",
    "        self.coeffs_attended = nn.Parameter(torch.empty(seq_length, input_dim))\n",
    "\n",
    "        # Learnable scalar offset for output adjustment\n",
    "        self.offset = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "        # Initialize model parameters\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"\n",
    "        Initializes model parameters using a normal distribution with a small standard deviation.\n",
    "        \"\"\"\n",
    "        init_scale = 0.04  # Standard deviation for parameter initialization\n",
    "\n",
    "        # Initialize all weight matrices and learnable parameters\n",
    "        for param in [self.query_matrix_1, self.key_matrix_1, self.value_matrix_1,\n",
    "                      self.query_matrix_2, self.key_matrix_2, self.value_matrix_2,\n",
    "                      self.query_matrix_3, self.key_matrix_3, self.value_matrix_3,\n",
    "                      self.random_matrix, self.coeffs_attended, self.offset]:\n",
    "            init.normal_(param, std=init_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the three-layer self-attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_length, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Final attended output of shape (batch_size,).\n",
    "        \"\"\"\n",
    "        # Apply random projection to the sequence data\n",
    "        y = torch.matmul(x, self.random_matrix)       \n",
    "\n",
    "        # Concatenate an additional constant feature\n",
    "        z = torch.cat((y, torch.ones(y.shape[0], y.shape[1], 1).to(device)), dim=2)\n",
    "\n",
    "        # First self-attention layer\n",
    "        query_1 = torch.matmul(z, self.query_matrix_1)\n",
    "        key_1 = torch.matmul(z, self.key_matrix_1)\n",
    "        value_1 = torch.matmul(z, self.value_matrix_1)\n",
    "        scores_1 = torch.matmul(query_1, key_1.transpose(1, 2)).softmax(dim=-1)  # Compute attention scores\n",
    "        attended_values_1 = torch.matmul(scores_1, value_1)  # Apply attention weights\n",
    "\n",
    "        # Second self-attention layer\n",
    "        query_2 = torch.matmul(attended_values_1, self.query_matrix_2)\n",
    "        key_2 = torch.matmul(attended_values_1, self.key_matrix_2)\n",
    "        value_2 = torch.matmul(attended_values_1, self.value_matrix_2)\n",
    "        scores_2 = torch.matmul(query_2, key_2.transpose(1, 2)).softmax(dim=-1)\n",
    "        attended_values_2 = torch.matmul(scores_2, value_2)\n",
    "\n",
    "        # Third self-attention layer\n",
    "        query_3 = torch.matmul(attended_values_2, self.query_matrix_3)\n",
    "        key_3 = torch.matmul(attended_values_2, self.key_matrix_3)\n",
    "        value_3 = torch.matmul(attended_values_2, self.value_matrix_3)\n",
    "        scores_3 = torch.matmul(query_3, key_3.transpose(1, 2)).softmax(dim=-1)\n",
    "        attended_values_3 = torch.matmul(scores_3, value_3)\n",
    "        \n",
    "        # Compute final weighted sum using learned coefficients\n",
    "        output = torch.einsum(\"bij,ij->b\", attended_values_3, self.coeffs_attended) + self.offset\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e67fd-c0cf-43ff-b75f-6c54c4aa289b",
   "metadata": {},
   "source": [
    "### Fitness generation, training, and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6bdef-8cab-4e59-a98f-cd4572cf7cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(filename, *args):\n",
    "    \"\"\"\n",
    "    Appends values to a file, separating them by spaces.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the output file.\n",
    "        *args: Values to write (e.g., low_dim, epoch, R² score).\n",
    "    \"\"\"\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(' '.join(map(str, args)) + '\\n')\n",
    "\n",
    "def generate_noise(shape, mean, std_dev):\n",
    "    \"\"\"Generates Gaussian noise with specified mean and standard deviation.\"\"\"\n",
    "    return np.random.normal(mean, std_dev, shape)\n",
    "\n",
    "def generate_4th_order_interactions(geno_data, order, num_combinations):\n",
    "    \"\"\"\n",
    "    Generates 4th order interaction terms from genotype data.\n",
    "\n",
    "    Args:\n",
    "        geno_data (ndarray): Genotype data of shape (n_samples, n_loci).\n",
    "        order (int): Order of interactions (e.g., 4 for 4th order).\n",
    "        num_combinations (int): Number of interaction terms to generate.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Generated interaction terms of shape (n_samples, num_combinations).\n",
    "    \"\"\"\n",
    "    n_samples, n_loci = geno_data.shape\n",
    "    interaction_terms = np.empty((n_samples, num_combinations))\n",
    "\n",
    "    selected_combinations = set()\n",
    "    i = 0\n",
    "    while i < num_combinations:\n",
    "        # Randomly select `order` loci without replacement\n",
    "        loci_indices = tuple(sorted(np.random.choice(n_loci, size=order, replace=False)))\n",
    "\n",
    "        # Ensure unique combinations\n",
    "        if loci_indices not in selected_combinations:\n",
    "            interaction_terms[:, i] = np.prod(geno_data[:, loci_indices], axis=1)\n",
    "            selected_combinations.add(loci_indices)\n",
    "            i += 1\n",
    "\n",
    "    return interaction_terms\n",
    "\n",
    "# Define scaling factor and reference temperature\n",
    "scale_all = 1e-2\n",
    "t0 = 30.0\n",
    "\n",
    "def calculate_fitness_with_interactions(geno_data, temps, num_combinations, e, order):\n",
    "    \"\"\"\n",
    "    Computes fitness with interactions, linear terms, and added noise.\n",
    "\n",
    "    Args:\n",
    "        geno_data (ndarray): Genotype data.\n",
    "        temps (ndarray): Temperature values.\n",
    "        num_combinations (int): Number of interaction terms.\n",
    "        e (float): Weight between linear and interaction terms.\n",
    "        order (int): Order of interactions.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Fitness values with noise.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    num_temps = temps.shape[0]\n",
    "\n",
    "    # Generate coefficients from exponential distributions\n",
    "    coeffs_const = np.random.exponential(1, num_loci)\n",
    "    coeffs_lin = np.random.exponential(1, num_loci)\n",
    "    coeffs_square = np.random.exponential(1, num_loci)\n",
    "\n",
    "    # Generate interaction coefficients\n",
    "    interaction_data = generate_4th_order_interactions(geno_data, order, num_combinations)\n",
    "    interaction_coeffs_const = np.random.exponential(1, interaction_data.shape[1])\n",
    "    interaction_coeffs_lin = np.random.exponential(1, interaction_data.shape[1])\n",
    "    interaction_coeffs_square = np.random.exponential(1, interaction_data.shape[1])\n",
    "\n",
    "    # Generate offset\n",
    "    offset = np.random.exponential(1, 1)\n",
    "\n",
    "    fitness = np.zeros((geno_data.shape[0], num_temps))\n",
    "\n",
    "    for i, temp in enumerate(temps):\n",
    "        linear_terms = scale_all * np.dot(\n",
    "            geno_data, coeffs_square * (temp - t0)**2 + coeffs_lin * (temp - t0) + coeffs_const\n",
    "        ) / num_loci\n",
    "\n",
    "        interaction_terms = scale_all * np.dot(\n",
    "            interaction_data, interaction_coeffs_square * (temp - t0)**2 +\n",
    "            interaction_coeffs_lin * (temp - t0) + interaction_coeffs_const\n",
    "        ) / interaction_data.shape[1]\n",
    "\n",
    "        y = e * linear_terms + (1 - e) * interaction_terms + scale_all * offset\n",
    "        fitness[:, i] = -1.0 * y + 1.0  # Adjust fitness scale\n",
    "\n",
    "    # Add noise to fitness data\n",
    "    fitness_with_noise = np.zeros(fitness.shape)\n",
    "    for i, temp in enumerate(temps):\n",
    "        std = 0.2 * np.mean(np.var(fitness[:, i])**0.5)\n",
    "        noise = generate_noise(fitness.shape[0], 0.0, std)\n",
    "        fitness_with_noise[:, i] = fitness[:, i] + noise\n",
    "\n",
    "    return fitness_with_noise\n",
    "\n",
    "# List of e values (weights for linear vs. interaction terms)\n",
    "e_list = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# Pick loci from BBQ genotype data\n",
    "genotype = pick_loci(num_loci)\n",
    "\n",
    "# List of dimensionality values to evaluate\n",
    "low_dims = [30, 50]\n",
    "\n",
    "# Iterate over different low-dimensional embeddings\n",
    "for low_dim in low_dims:\n",
    "    \n",
    "    save_dir = f\"./synthetic_data_attention_exponential/exp/num_loci_{num_loci}/d_{low_dim}\"\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # File to store test R² scores\n",
    "    filename2 = f\"{save_dir}/test_r2_scores.txt\"\n",
    "    if os.path.exists(filename2):\n",
    "        os.remove(filename2)\n",
    "\n",
    "    # Iterate over different values of e (linear vs. interaction term weight)\n",
    "    for e in e_list:\n",
    "\n",
    "        # Generate synthetic fitness data with noise\n",
    "        fitness_with_noise = calculate_fitness_with_interactions(genotype, temps, num_loci, e, 4)\n",
    "\n",
    "        # Define model input dimensions\n",
    "        seq_length = num_loci\n",
    "        input_dim = low_dim + 1\n",
    "        query_dim = low_dim + 1\n",
    "        key_dim = low_dim + 1\n",
    "\n",
    "        # File to store validation R² scores\n",
    "        filename = f\"{save_dir}/validation_r2.txt\"\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "\n",
    "        # Set computation device (GPU if available)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Split dataset into training (85%) and testing (15%)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(genotype, fitness_with_noise, test_size=0.15, random_state=42)\n",
    "        X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "        # Compute mean and standard deviation for normalization\n",
    "        mean_values = np.nanmean(y_train2, axis=0)\n",
    "        std_values = np.nanstd(y_train2, axis=0)\n",
    "\n",
    "        # Normalize target fitness data\n",
    "        y_train2 = (y_train2 - mean_values) / std_values\n",
    "        y_val = (y_val - mean_values) / std_values\n",
    "        y_test = (y_test - mean_values) / std_values\n",
    "\n",
    "        # Convert dataset to PyTorch tensors\n",
    "        X_train_tens = torch.tensor(X_train2).float()\n",
    "        y_train_tens = torch.tensor(np.array(y_train2)).float()\n",
    "\n",
    "        # Initialize the attention model and move it to CUDA\n",
    "        attention_layer = ThreeLayerAttention(input_dim, query_dim, key_dim, seq_length).to(device)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        loss_function = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(attention_layer.parameters(), lr=0.001)\n",
    "\n",
    "        # Define training parameters\n",
    "        batch_size = 64\n",
    "        chunk_size = 100\n",
    "        num_epochs = 1000 if low_dim == 2 else 500\n",
    "        num_batches = X_train_tens.size(0) // batch_size\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            # Shuffle training data at the beginning of each epoch\n",
    "            indices = torch.randperm(X_train_tens.size(0))\n",
    "            train_input_shuffled = X_train_tens[indices]\n",
    "            train_target_shuffled = y_train_tens[indices]\n",
    "\n",
    "            # Mini-batch training\n",
    "            for i in range(num_batches):\n",
    "\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "\n",
    "                mini_batch_input = train_input_shuffled[start_idx:end_idx].to(device)\n",
    "                mini_batch_target = train_target_shuffled[start_idx:end_idx].squeeze(1).to(device)\n",
    "\n",
    "                # Create one-hot encoding for genotype data\n",
    "                one_hot_mini_batch_input = torch.zeros((mini_batch_input.shape[0], num_loci, num_loci), device=device)\n",
    "                indices = torch.arange(num_loci, device=device)\n",
    "                one_hot_mini_batch_input[:, indices, indices] = mini_batch_input.squeeze()\n",
    "\n",
    "                # Forward pass\n",
    "                train_output = attention_layer(one_hot_mini_batch_input).to(device)\n",
    "                train_loss = loss_function(train_output, mini_batch_target)\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Save model parameters after each epoch\n",
    "            model_state_path = os.path.join(save_dir, f\"epoch_{epoch}.pt\")\n",
    "            torch.save(attention_layer.state_dict(), model_state_path)\n",
    "\n",
    "            # VALIDATION EVALUATION\n",
    "            y_pred = torch.tensor([]).to(device)\n",
    "            y_val_all = np.array([])\n",
    "\n",
    "            for env in range(temps.shape[0]):\n",
    "                y_val_env = y_val.T[env]\n",
    "                X_val_tens = torch.tensor(np.array(X_val)).float().to(device)\n",
    "\n",
    "                # Process validation data in chunks to prevent memory issues\n",
    "                for i in range(0, len(X_val_tens), chunk_size):\n",
    "                    chunk = X_val_tens[i:i + chunk_size].to(device)\n",
    "                    chunk_size_actual = min(chunk_size, len(X_val_tens) - i)\n",
    "\n",
    "                    one_hot_val_input = torch.zeros((chunk_size_actual, num_loci, num_loci), device=device)\n",
    "                    indices = torch.arange(num_loci, device=device)\n",
    "                    one_hot_val_input[:, indices, indices] = chunk.squeeze(dim=1)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        i_pred = attention_layer(one_hot_val_input)\n",
    "\n",
    "                    y_pred = torch.cat((y_pred, i_pred), dim=0)\n",
    "\n",
    "                y_val_all = np.concatenate((y_val_all, y_val_env))\n",
    "\n",
    "            val_r_squared = r2_score(y_val_all, y_pred.cpu())\n",
    "\n",
    "            # Save validation R² score\n",
    "            write_to_file(filename, low_dim, epoch, val_r_squared)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # LOAD THE BEST MODEL CHECKPOINT\n",
    "        data = pd.read_csv(filename, sep='\\s+', header=None)\n",
    "        max_row_index = data[2].idxmax()\n",
    "        max_row = data.loc[max_row_index]\n",
    "        max_second_column_value = max_row[1]\n",
    "\n",
    "        # Initialize a new attention model\n",
    "        attention_layer = ThreeLayerAttention(input_dim, query_dim, key_dim, seq_length).to(device)\n",
    "\n",
    "        # Load best-performing model\n",
    "        epoch = int(max_second_column_value)\n",
    "        model_path = f\"{save_dir}/epoch_{epoch}.pt\"\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "        attention_layer.load_state_dict(state_dict)\n",
    "        attention_layer.to(device)\n",
    "        attention_layer.eval()\n",
    "\n",
    "        # TEST SET EVALUATION\n",
    "        for env in range(temps.shape[0]):\n",
    "\n",
    "            y_pred = torch.tensor([]).to(device)\n",
    "            y_test_env = y_test.T[env]\n",
    "            X_test_tens = torch.tensor(np.array(X_test)).float().to(device)\n",
    "\n",
    "            # Process test data in chunks\n",
    "            for i in range(0, len(X_test_tens), chunk_size):\n",
    "                chunk = X_test_tens[i:i + chunk_size].to(device)\n",
    "                chunk_size_actual = min(chunk_size, len(X_test_tens) - i)\n",
    "\n",
    "                one_hot_test_input = torch.zeros((chunk_size_actual, num_loci, num_loci), device=device)\n",
    "                indices = torch.arange(num_loci, device=device)\n",
    "                one_hot_test_input[:, indices, indices] = chunk.squeeze(dim=1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    i_pred = attention_layer(one_hot_test_input)\n",
    "\n",
    "                y_pred = torch.cat((y_pred, i_pred), dim=0)\n",
    "\n",
    "            # Compute test R² score\n",
    "            test_r_squared = r2_score(y_test_env, y_pred.cpu())\n",
    "            write_to_file(filename2, num_loci, e, temps[env], test_r_squared)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
