{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7189319-9f83-4f11-b913-81bda0a81bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d598d1-a6a2-4e9e-96a5-0e4bb065b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the genotype data\n",
    "geno_data = np.load(\"merged_geno_data.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453a504-07f9-4a1d-b977-3b1d468aa5ba",
   "metadata": {},
   "source": [
    "### Genotype data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b33803a-5ff9-42d6-9438-beab854852e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature conditions\n",
    "temps = np.array([23.0, 25.0, 27.0, 29.0, 31.0, 33.0, 35.0, 37.0])\n",
    "\n",
    "def pick_loci(num_loci):\n",
    "    \"\"\"\n",
    "    Selects a subset of loci that meet a threshold based on mean allele frequency.\n",
    "\n",
    "    Args:\n",
    "        num_loci (int): The number of loci to select.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: A genotype matrix of shape (num_segregants, num_loci) \n",
    "                 with values mapped from [0,1] to [-1,+1].\n",
    "    \"\"\"\n",
    "    # Get the number of segregants (samples)\n",
    "    num_segregants = geno_data.shape[0]\n",
    "\n",
    "    # Shuffle segregant indices for randomization\n",
    "    shuffled_indices = list(range(num_segregants))\n",
    "    random.seed(0)  # Ensure reproducibility\n",
    "    random.shuffle(shuffled_indices)\n",
    "\n",
    "    # Apply shuffling to genotype data\n",
    "    geno_data2 = geno_data[shuffled_indices]\n",
    "\n",
    "    # Load independent loci list\n",
    "    loci_list = np.load(\"ind_loci_list_3.npy\")\n",
    "    loci_list = np.sort(loci_list)  # Sort loci indices in ascending order\n",
    "    loci_list_reduced = []  # List to store selected loci\n",
    "\n",
    "    # Iterate through loci and filter based on mean allele frequency\n",
    "    for i in loci_list:\n",
    "        avg_loci = (2.0 * geno_data2[:, i] - 1.0).mean()  # Convert allele states to [-1, 1] and compute mean\n",
    "        \n",
    "        if abs(avg_loci) < 0.05:  # Select loci with mean close to zero (balanced representation)\n",
    "            loci_list_reduced.append(i)\n",
    "        \n",
    "        if len(loci_list_reduced) > num_loci - 1:  # Stop when required number of loci is reached\n",
    "            break\n",
    "\n",
    "    # Extract the genotype data for selected loci and map values from [0,1] to [-1,+1]\n",
    "    genotype = 2.0 * geno_data2[:, np.array(loci_list_reduced)] - 1.0\n",
    "    \n",
    "    return genotype\n",
    "\n",
    "# Define number of loci to be selected\n",
    "num_loci = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be760bb-0cf6-4d3a-abbe-e545f246acbc",
   "metadata": {},
   "source": [
    "### Attention layer class in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6bcec0-fe17-4fae-bffa-ec22978b2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerAttention(nn.Module):\n",
    "    def __init__(self, input_dim, query_dim, key_dim, seq_length):\n",
    "        \"\"\"\n",
    "        Implements a three-layer attention model for processing input sequences with environmental embeddings.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimensionality of input features.\n",
    "            query_dim (int): Dimensionality of query vectors.\n",
    "            key_dim (int): Dimensionality of key vectors.\n",
    "            seq_length (int): Sequence length, including additional environmental embeddings.\n",
    "        \"\"\"\n",
    "        super(ThreeLayerAttention, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim  # Input feature dimension\n",
    "        self.query_dim = query_dim  # Query vector dimension\n",
    "        self.key_dim = key_dim  # Key vector dimension\n",
    "        self.seq_length = seq_length  # Length of input sequence\n",
    "\n",
    "        # Learnable query, key, and value matrices for the first attention layer\n",
    "        self.query_matrix_1 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_1 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_1 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "\n",
    "        # Learnable query, key, and value matrices for the second attention layer\n",
    "        self.query_matrix_2 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_2 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_2 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "\n",
    "        # Learnable query, key, and value matrices for the third attention layer\n",
    "        self.query_matrix_3 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_3 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_3 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "\n",
    "        # Learnable random matrix for projecting input embeddings\n",
    "        self.random_matrix = nn.Parameter(torch.empty(seq_length - 1, low_dim))\n",
    "\n",
    "        # Learnable coefficients for final attended values\n",
    "        self.coeffs_attended = nn.Parameter(torch.empty(seq_length, input_dim))\n",
    "        \n",
    "        # Learnable bias offset applied to final output\n",
    "        self.offset = nn.Parameter(torch.randn(1))\n",
    "\n",
    "        # Initialize model parameters\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"\n",
    "        Initializes model parameters using a normal distribution with a small standard deviation.\n",
    "        \"\"\"\n",
    "        init_scale = 0.04\n",
    "\n",
    "        # Apply normal initialization to all learnable parameters\n",
    "        for param in [self.query_matrix_1, self.key_matrix_1, self.value_matrix_1,\n",
    "                      self.query_matrix_2, self.key_matrix_2, self.value_matrix_2,\n",
    "                      self.query_matrix_3, self.key_matrix_3, self.value_matrix_3,\n",
    "                      self.random_matrix, self.coeffs_attended, self.offset]:\n",
    "            init.normal_(param, std=init_scale)\n",
    "\n",
    "    def forward(self, x, envs_one_hot):\n",
    "        \"\"\"\n",
    "        Forward pass through the three-layer attention model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_length-1, input_dim).\n",
    "            envs_one_hot (Tensor): One-hot encoded environmental embeddings, shape (batch_size, low_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Final output prediction of shape (batch_size,).\n",
    "        \"\"\"\n",
    "\n",
    "        # Project input features using the learnable random matrix\n",
    "        y = torch.matmul(x, self.random_matrix)\n",
    "\n",
    "        # Concatenate projected input with one-hot environmental embeddings\n",
    "        z = torch.cat((y, envs_one_hot.unsqueeze(1)), dim=1)\n",
    "\n",
    "        # Append an additional bias term of ones\n",
    "        z = torch.cat((z, torch.ones(z.shape[0], z.shape[1], 1).to(device)), dim=2)\n",
    "\n",
    "        ### First attention layer ###\n",
    "        query_1 = torch.matmul(z, self.query_matrix_1)  # Compute queries\n",
    "        key_1 = torch.matmul(z, self.key_matrix_1)  # Compute keys\n",
    "        value_1 = torch.matmul(z, self.value_matrix_1)  # Compute values\n",
    "\n",
    "        scores_1 = torch.matmul(query_1, key_1.transpose(1, 2)).softmax(dim=-1)  # Compute attention scores\n",
    "        attended_values_1 = torch.matmul(scores_1, value_1)  # Apply attention to values\n",
    "\n",
    "        ### Second attention layer ###\n",
    "        query_2 = torch.matmul(attended_values_1, self.query_matrix_2)\n",
    "        key_2 = torch.matmul(attended_values_1, self.key_matrix_2)\n",
    "        value_2 = torch.matmul(attended_values_1, self.value_matrix_2)\n",
    "\n",
    "        scores_2 = torch.matmul(query_2, key_2.transpose(1, 2)).softmax(dim=-1)\n",
    "        attended_values_2 = torch.matmul(scores_2, value_2)\n",
    "\n",
    "        ### Third attention layer ###\n",
    "        query_3 = torch.matmul(attended_values_2, self.query_matrix_3)\n",
    "        key_3 = torch.matmul(attended_values_2, self.key_matrix_3)\n",
    "        value_3 = torch.matmul(attended_values_2, self.value_matrix_3)\n",
    "\n",
    "        scores_3 = torch.matmul(query_3, key_3.transpose(1, 2)).softmax(dim=-1)\n",
    "        attended_values_3 = torch.matmul(scores_3, value_3)\n",
    "\n",
    "        # Compute final output using learned coefficients and add bias offset\n",
    "        output = torch.einsum(\"bij,ij->b\", attended_values_3, self.coeffs_attended) + self.offset\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01845ae0-5e35-47b9-afcb-d3a832346ede",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define function for fourier embedding of the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8012e5f3-7a83-47a4-8fba-66b2e19515b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_temp_embeddings(temps):\n",
    "    \"\"\"\n",
    "    Generate one-hot embeddings for temperatures with random shuffling and pad them with zeros.\n",
    "\n",
    "    Args:\n",
    "    - temps (Tensor): A 1D tensor of temperature values.\n",
    "    - low_dim (int): The target dimensionality (default is 30).\n",
    "    - seed (int): The random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: A 2D tensor of shape [len(temps), low_dim] containing padded one-hot embeddings.\n",
    "    \"\"\"\n",
    "    # Define the list of unique temperatures\n",
    "    unique_temps = [23.0, 25.0, 27.0, 29.0, 31.0, 33.0, 35.0, 37.0]\n",
    "\n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    shuffled_temps = random.sample(unique_temps, len(unique_temps))  # Shuffle the temperatures\n",
    "\n",
    "    # Create a mapping from temperature to one-hot index\n",
    "    temp_to_index = {temp: idx for idx, temp in enumerate(shuffled_temps)}\n",
    "\n",
    "    # Initialize a tensor to hold the one-hot vectors\n",
    "    batch_size = len(temps)\n",
    "    one_hot_vectors = torch.zeros((batch_size, len(unique_temps)))\n",
    "\n",
    "    # Set the appropriate positions to 1 based on the shuffled temperature values\n",
    "    for i, temp in enumerate(temps):\n",
    "        if temp.item() in temp_to_index:\n",
    "            one_hot_vectors[i, temp_to_index[temp.item()]] = 1.0\n",
    "\n",
    "    # Pad with zeros to match low_dim (30)\n",
    "    padding = torch.zeros((batch_size, low_dim - len(unique_temps)))\n",
    "    one_hot_padded = torch.cat((one_hot_vectors, padding), dim=1)\n",
    "\n",
    "    return one_hot_padded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e67fd-c0cf-43ff-b75f-6c54c4aa289b",
   "metadata": {},
   "source": [
    "### Training on all temperatures combined, using $m$ training data at $T = 23$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1448aa-4bb2-4dc9-a587-f4f32bd2891f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define directory for saving results\n",
    "save_dir = \"./transfer_learning_synthetic_temps/23C\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Define filename for storing test R² scores and remove it if it exists\n",
    "filename2 = f\"{save_dir}/test_r2_score_vs_epsilon.txt\"\n",
    "if os.path.exists(filename2):\n",
    "    os.remove(filename2)\n",
    "\n",
    "def write_to_file(filename, *args):\n",
    "    \"\"\"\n",
    "    Writes space-separated values to a specified file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): File path to store results.\n",
    "        *args: Values to be written in the file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(' '.join(map(str, args)) + '\\n')\n",
    "\n",
    "def generate_noise(shape, mean, std_dev):\n",
    "    \"\"\"\n",
    "    Generates Gaussian noise.\n",
    "\n",
    "    Args:\n",
    "        shape (tuple): Shape of the noise array.\n",
    "        mean (float): Mean of the Gaussian distribution.\n",
    "        std_dev (float): Standard deviation of the Gaussian distribution.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Gaussian noise array of the specified shape.\n",
    "    \"\"\"\n",
    "    return np.random.normal(mean, std_dev, shape)\n",
    "\n",
    "def generate_interactions(geno_data, order, num_combinations):\n",
    "    \"\"\"\n",
    "    Generates `order`-order interaction terms for genotype data with a specified number of random `order` combinations.\n",
    "\n",
    "    Args:\n",
    "        geno_data (np.ndarray): The original genotype data, shape = (n_samples, n_loci).\n",
    "        order (int): Interaction order (e.g., 2 for pairwise, 4 for fourth-order).\n",
    "        num_combinations (int): Number of unique random 'order' combinations to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Interaction terms of shape (n_samples, num_combinations).\n",
    "    \"\"\"\n",
    "    n_samples, n_loci = geno_data.shape  # Get number of samples and loci\n",
    "    interaction_terms = np.empty((n_samples, num_combinations))  # Initialize array\n",
    "\n",
    "    # Track selected combinations to ensure uniqueness\n",
    "    selected_combinations = set()\n",
    "\n",
    "    i = 0  # Counter for filling interaction terms\n",
    "    while i < num_combinations:\n",
    "        # Select `order` loci indices without replacement\n",
    "        loci_indices = tuple(sorted(np.random.choice(n_loci, size=order, replace=False)))\n",
    "\n",
    "        # Ensure uniqueness before adding the interaction term\n",
    "        if loci_indices not in selected_combinations:\n",
    "            interaction_terms[:, i] = np.prod(geno_data[:, loci_indices], axis=1)  # Compute interaction product\n",
    "            selected_combinations.add(loci_indices)  # Add combination to the set\n",
    "            i += 1  # Move to the next combination\n",
    "\n",
    "    return interaction_terms  # Return generated interaction terms\n",
    "\n",
    "# Define constants for fitness calculation\n",
    "mean1 = 0.5  # Mean for coefficient sampling\n",
    "std1 = 0.5  # Standard deviation for coefficient sampling\n",
    "scale = 1e-2  # Scaling factor for fitness components\n",
    "t0 = 30.0  # Reference temperature\n",
    "\n",
    "def calculate_fitness_with_interactions(geno_data, temps, num_loci, e, order):\n",
    "    \"\"\"\n",
    "    Calculates fitness scores based on genotype data with added noise.\n",
    "    The function includes:\n",
    "      - Constant genetic effects\n",
    "      - Linear effects\n",
    "      - Higher-order (4th-order) interactions\n",
    "\n",
    "    Args:\n",
    "        geno_data (np.ndarray): Genotype data of shape (n_samples, n_loci).\n",
    "        temps (np.ndarray): Array of environmental temperatures.\n",
    "        num_loci (int): Number of loci considered.\n",
    "        e (float): Proportion of linear vs interaction contributions (e=1 → all linear, e=0 → all interactions).\n",
    "        order (int): Interaction order (e.g., 4 for fourth-order interactions).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Fitness values with added noise, shape (n_samples, len(temps)).\n",
    "    \"\"\"\n",
    "    np.random.seed(0)  # Set seed for reproducibility\n",
    "    num_temps = temps.shape[0]  # Number of environmental conditions (temperatures)\n",
    "\n",
    "    # Generate random coefficients for constant, linear, and quadratic effects\n",
    "    coeffs_const = np.random.normal(mean1, std1, num_loci)  \n",
    "    coeffs_lin = np.random.normal(mean1, std1, num_loci)  \n",
    "    coeffs_square = np.random.normal(mean1, std1, num_loci)\n",
    "\n",
    "    # Generate higher-order interaction terms from genotype data\n",
    "    interaction_data = generate_interactions(geno_data, order, num_loci)\n",
    "\n",
    "    # Generate random coefficients for interactions\n",
    "    interaction_coeffs_const = np.random.normal(mean1, std1, interaction_data.shape[1])  \n",
    "    interaction_coeffs_lin = np.random.normal(mean1, std1, interaction_data.shape[1]) \n",
    "    interaction_coeffs_square = np.random.normal(mean1, std1, interaction_data.shape[1]) \n",
    "\n",
    "    # Generate a random offset term for baseline fitness adjustment\n",
    "    offset = np.random.normal(0, 1, 1)  \n",
    "\n",
    "    # Initialize fitness array with zeros, shape: (n_samples, num_temps)\n",
    "    fitness = np.zeros((geno_data.shape[0], num_temps))\n",
    "\n",
    "    # Compute fitness values for each temperature condition\n",
    "    for i, temp in enumerate(temps):\n",
    "        # Compute linear genetic effects, with quadratic dependency on temperature\n",
    "        linear_terms = scale * np.dot(\n",
    "            geno_data, \n",
    "            coeffs_square * (temp - t0) ** 2 + coeffs_lin * (temp - t0) + coeffs_const\n",
    "        ) / num_loci\n",
    "\n",
    "        # Compute interaction effects, also temperature-dependent\n",
    "        interaction_terms = scale * np.dot(\n",
    "            interaction_data, \n",
    "            interaction_coeffs_square * (temp - t0) ** 2 + interaction_coeffs_lin * (temp - t0) + interaction_coeffs_const\n",
    "        ) / interaction_data.shape[1]\n",
    "\n",
    "        # Compute weighted sum of linear and interaction effects\n",
    "        y = e * linear_terms + (1 - e) * interaction_terms + scale * offset\n",
    "\n",
    "        # Adjust fitness values so that higher fitness is represented by positive values\n",
    "        fitness[:, i] = -1.0 * y + 1.0  \n",
    "\n",
    "    # Add Gaussian noise to the fitness data\n",
    "    fitness_with_noise = np.zeros(fitness.shape)\n",
    "\n",
    "    for i, temp in enumerate(temps):\n",
    "        # Compute noise standard deviation as 20% of the standard deviation of fitness\n",
    "        std = 0.2 * np.mean(np.var(fitness[:, i]) ** 0.5)\n",
    "        \n",
    "        # Generate noise for the fitness values\n",
    "        noise = generate_noise(fitness.shape[0], 0.0, std)\n",
    "        \n",
    "        # Add noise to the original fitness values\n",
    "        fitness_with_noise[:, i] = fitness[:, i] + noise\n",
    "\n",
    "    return fitness_with_noise  # Return the noisy fitness matrix\n",
    "\n",
    "# Set epsilon value for fitness computation\n",
    "e = 0.3\n",
    "\n",
    "# Generate genotype data by selecting loci\n",
    "genotype = pick_loci(num_loci)\n",
    "\n",
    "# Generate synthetic fitness data with noise based on interactions\n",
    "fitness_with_noise = calculate_fitness_with_interactions(genotype, temps, num_loci, e, 4)\n",
    "\n",
    "# Define model hyperparameters\n",
    "low_dim = 30  # Dimensionality of the Fourier embeddings\n",
    "seq_length = num_loci + 1  # Sequence length\n",
    "input_dim = low_dim + 1  # Input dimension\n",
    "query_dim = low_dim + 1  # Query vector dimension\n",
    "key_dim = low_dim + 1  # Key vector dimension\n",
    "hidden_dim = 32  # Hidden layer size for the MLPs\n",
    "\n",
    "# Move the model and tensors to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Filter indices for temperatures 23°C\n",
    "temp_indices = [i for i, temp in enumerate(temps) if temp in [23.0]]\n",
    "\n",
    "# First split: Training and Testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(genotype, fitness_with_noise, test_size=0.15, random_state=42)\n",
    "\n",
    "# Define a range for the number of training samples m \n",
    "m_values = np.logspace(1, np.log10(len(X_train)), num=5, dtype=int)\n",
    "\n",
    "# Iterate over different values of m \n",
    "for m in m_values:\n",
    "    \n",
    "    # Define filename for validation R² scores\n",
    "    filename = f\"{save_dir}/validation_r2.txt\"\n",
    "\n",
    "    # Remove the file if it exists to start fresh\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "\n",
    "    # Create a modified copy of y_train to avoid modifying the original dataset\n",
    "    y_train_modified = y_train.copy()\n",
    "\n",
    "    # Set all fitness values to NaN for the selected temperatures, except for the first m samples\n",
    "    for idx in temp_indices:\n",
    "        y_train_modified[m:, idx] = np.nan  # Retain first m samples, set rest to NaN\n",
    "\n",
    "    # Second split: Split training data further into training and validation sets\n",
    "    X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train_modified, test_size=0.15, random_state=42)\n",
    "\n",
    "    # Compute mean and standard deviation for normalization\n",
    "    mean_values = np.nanmean(y_train2, axis=0)\n",
    "    std_values = np.nanstd(y_train2, axis=0)\n",
    "\n",
    "    # Normalize training, validation, and test sets using computed mean and std\n",
    "    y_train2 = (y_train2 - mean_values) / std_values\n",
    "    y_val = (y_val - mean_values) / std_values\n",
    "    y_test = (y_test - mean_values) / std_values\n",
    "\n",
    "    # Convert training data to PyTorch tensors\n",
    "    X_train_tens = torch.tensor(X_train2).float()\n",
    "    y_train_tens = torch.tensor(np.array(y_train2)).float()\n",
    "\n",
    "    # Initialize the three-layer attention model and move it to CUDA if available\n",
    "    attention_layer = ThreeLayerAttention(input_dim, query_dim, key_dim, seq_length).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(attention_layer.parameters(), lr=0.001)\n",
    "\n",
    "    # Define batch parameters\n",
    "    num_temps = temps.shape[0]  # Number of environmental conditions\n",
    "    num_elements = int(64 / num_temps)  # Number of elements per batch per environment\n",
    "    batch_size = num_temps * num_elements  # Total batch size\n",
    "    chunk_size = 100  # Chunk size for validation inference\n",
    "    num_epochs = 500  # Number of training epochs\n",
    "    num_batches = X_train_tens.size(0) // batch_size  # Compute number of batches per epoch\n",
    "\n",
    "    ### **TRAINING LOOP**\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Shuffle training data before each epoch\n",
    "        indices = torch.randperm(X_train_tens.size(0))\n",
    "        train_input_shuffled = X_train_tens[indices]\n",
    "        train_target_shuffled = y_train_tens[indices].T  # Transpose for easier environment access\n",
    "\n",
    "        # Mini-batch training\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "\n",
    "            fitness_env = []\n",
    "\n",
    "            # Collect num_elements number of fitness data from each environment\n",
    "            for j, row in enumerate(train_target_shuffled):\n",
    "                start_col = start_idx + j * num_elements\n",
    "                end_col = start_col + num_elements\n",
    "                fitness_env.extend(row[start_col:end_col])\n",
    "\n",
    "            mini_batch_input = train_input_shuffled[start_idx:end_idx]\n",
    "            mini_batch_target = torch.tensor(fitness_env)\n",
    "\n",
    "            # Identify and remove NaN values from mini-batch targets\n",
    "            nan_mask = np.isnan(mini_batch_target)\n",
    "            nan_indices = np.where(nan_mask)[0]\n",
    "            mini_batch_input = np.delete(mini_batch_input, nan_indices, axis=0).to(device)\n",
    "            mini_batch_target = np.delete(mini_batch_target, nan_indices).to(device)\n",
    "\n",
    "            # Generate one-hot environment embeddings\n",
    "            envs = temps.repeat(num_elements)\n",
    "            envs = np.delete(envs, nan_indices)\n",
    "            envs = torch.tensor(envs)\n",
    "            envs_one_hot = one_hot_temp_embeddings(envs).to(device)\n",
    "\n",
    "            # Create one-hot vector embedding for the loci\n",
    "            one_hot_mini_batch_input = torch.zeros((mini_batch_input.shape[0], num_loci, num_loci), device=device)\n",
    "            indices = torch.arange(num_loci, device=device)\n",
    "            one_hot_mini_batch_input[:, indices, indices] = mini_batch_input.squeeze()\n",
    "\n",
    "            # Forward pass for the mini-batch\n",
    "            train_output = attention_layer(one_hot_mini_batch_input, envs_one_hot).to(device)\n",
    "            train_loss = loss_function(train_output, mini_batch_target)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Save the model parameters after each epoch\n",
    "        model_state_path = os.path.join(save_dir, f\"epoch_{epoch}.pt\")\n",
    "        torch.save(attention_layer.state_dict(), model_state_path)\n",
    "\n",
    "        ### **VALIDATION PHASE**\n",
    "        y_pred = torch.tensor([]).to(device)\n",
    "        y_val_all = np.array([])\n",
    "\n",
    "        # Iterate over all environmental conditions\n",
    "        for env in range(num_temps):\n",
    "\n",
    "            y_val_env = y_val.T[env]  # Extract validation target values\n",
    "\n",
    "            # Identify and remove NaN values from validation targets\n",
    "            nan_mask = np.isnan(y_val_env)\n",
    "            nan_indices = np.where(nan_mask)[0]\n",
    "            X_val_env = np.delete(X_val, nan_indices, axis=0)\n",
    "            y_val_env = np.delete(y_val_env, nan_indices, axis=0)\n",
    "\n",
    "            # Convert validation genotype data to PyTorch tensor\n",
    "            X_val_tens = torch.tensor(np.array(X_val_env)).float().to(device)\n",
    "\n",
    "            # Process validation data in chunks to avoid memory issues\n",
    "            for i in range(0, len(X_val_tens), chunk_size):\n",
    "                chunk = X_val_tens[i:i + chunk_size].to(device)\n",
    "                chunk_size_actual = min(chunk_size, len(X_val_tens) - i)\n",
    "\n",
    "                # Generate environment embeddings\n",
    "                envs = [temps[env] for _ in range(chunk_size_actual)]\n",
    "                envs = torch.tensor(envs)\n",
    "                envs_one_hot = one_hot_temp_embeddings(envs).to(device)\n",
    "\n",
    "                # Create one-hot encoded input representation\n",
    "                one_hot_val_input = torch.zeros((chunk_size_actual, num_loci, num_loci), device=device)\n",
    "                indices = torch.arange(num_loci, device=device)\n",
    "                one_hot_val_input[:, indices, indices] = chunk.squeeze(dim=1)\n",
    "\n",
    "                # Perform model inference without computing gradients\n",
    "                with torch.no_grad():\n",
    "                    i_pred = attention_layer(one_hot_val_input, envs_one_hot)\n",
    "\n",
    "                y_pred = torch.cat((y_pred, i_pred), dim=0)\n",
    "            y_val_all = np.concatenate((y_val_all, y_val_env))\n",
    "\n",
    "        # Compute R² score for validation predictions\n",
    "        val_r_squared = r2_score(y_val_all, y_pred.cpu())\n",
    "\n",
    "        # Save validation performance after each epoch\n",
    "        write_to_file(filename, low_dim, epoch, val_r_squared)\n",
    "        torch.cuda.empty_cache()  # Free up memory\n",
    "\n",
    "    ### **MODEL SELECTION: CHOOSE BEST EPOCH BASED ON VALIDATION PERFORMANCE**\n",
    "    data = pd.read_csv(filename, sep='\\s+', header=None)\n",
    "    max_row_index = data[2].idxmax()  # Find the index of the max R² score\n",
    "    max_row = data.loc[max_row_index]  # Extract the row with the best epoch\n",
    "    best_epoch = int(max_row[1])  # Extract best epoch number\n",
    "\n",
    "    # Copy the best-performing model to a dedicated file\n",
    "    model_path = f\"{save_dir}/epoch_{best_epoch}.pt\"\n",
    "    shutil.copyfile(model_path, f\"{save_dir}/best_model_num_data_{m}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
