{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e09d6dd",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7189319-9f83-4f11-b913-81bda0a81bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d598d1-a6a2-4e9e-96a5-0e4bb065b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the genotype data\n",
    "geno_data = np.load(\"merged_geno_data.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458ea8d-2add-4dd6-b97d-413c0fd30943",
   "metadata": {},
   "source": [
    "### Genotype data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b33803a-5ff9-42d6-9438-beab854852e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature conditions\n",
    "temps = np.array([23.0, 25.0, 27.0, 29.0, 31.0, 33.0, 35.0, 37.0])\n",
    "\n",
    "def pick_loci(num_loci):\n",
    "    \"\"\"\n",
    "    Selects a subset of loci that meet a threshold based on mean allele frequency.\n",
    "\n",
    "    Args:\n",
    "        num_loci (int): The number of loci to select.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: A genotype matrix of shape (num_segregants, num_loci) \n",
    "                 with values mapped from [0,1] to [-1,+1].\n",
    "    \"\"\"\n",
    "    # Get the number of segregants (samples)\n",
    "    num_segregants = geno_data.shape[0]\n",
    "\n",
    "    # Shuffle segregant indices for randomization\n",
    "    shuffled_indices = list(range(num_segregants))\n",
    "    random.seed(0)  # Ensure reproducibility\n",
    "    random.shuffle(shuffled_indices)\n",
    "\n",
    "    # Apply shuffling to genotype data\n",
    "    geno_data2 = geno_data[shuffled_indices]\n",
    "\n",
    "    # Load independent loci list\n",
    "    loci_list = np.load(\"ind_loci_list_3.npy\")\n",
    "    loci_list = np.sort(loci_list)  # Sort loci indices in ascending order\n",
    "    loci_list_reduced = []  # List to store selected loci\n",
    "\n",
    "    # Iterate through loci and filter based on mean allele frequency\n",
    "    for i in loci_list:\n",
    "        avg_loci = (2.0 * geno_data2[:, i] - 1.0).mean()  # Convert allele states to [-1, 1] and compute mean\n",
    "        \n",
    "        if abs(avg_loci) < 0.05:  # Select loci with mean close to zero (balanced representation)\n",
    "            loci_list_reduced.append(i)\n",
    "        \n",
    "        if len(loci_list_reduced) > num_loci - 1:  # Stop when required number of loci is reached\n",
    "            break\n",
    "\n",
    "    # Extract the genotype data for selected loci and map values from [0,1] to [-1,+1]\n",
    "    genotype = 2.0 * geno_data2[:, np.array(loci_list_reduced)] - 1.0\n",
    "    \n",
    "    return genotype\n",
    "\n",
    "# Define number of loci to be selected\n",
    "num_loci = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be760bb-0cf6-4d3a-abbe-e545f246acbc",
   "metadata": {},
   "source": [
    "### Attention layer class in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6bcec0-fe17-4fae-bffa-ec22978b2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerAttention(nn.Module):\n",
    "    def __init__(self, input_dim, query_dim, key_dim, seq_length, low_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Three-layer attention model with MLP processing for environmental Fourier features.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimension of input features.\n",
    "            query_dim (int): Dimension of query vectors.\n",
    "            key_dim (int): Dimension of key vectors.\n",
    "            seq_length (int): Length of the sequence.\n",
    "            low_dim (int): Dimensionality of the processed environmental embeddings.\n",
    "            hidden_dim (int): Hidden layer size for the MLPs.\n",
    "        \"\"\"\n",
    "        super(ThreeLayerAttention, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim  \n",
    "        self.query_dim = query_dim  \n",
    "        self.key_dim = key_dim  \n",
    "        self.seq_length = seq_length  \n",
    "        self.low_dim = low_dim  \n",
    "        self.hidden_dim = hidden_dim  \n",
    "\n",
    "        # Learnable parameters for attention layers\n",
    "        self.query_matrix_1 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_1 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_1 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "\n",
    "        self.query_matrix_2 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_2 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_2 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "        \n",
    "        self.query_matrix_3 = nn.Parameter(torch.empty(input_dim, query_dim))\n",
    "        self.key_matrix_3 = nn.Parameter(torch.empty(input_dim, key_dim))\n",
    "        self.value_matrix_3 = nn.Parameter(torch.empty(input_dim, input_dim))\n",
    "        \n",
    "        # Learnable random projection matrix\n",
    "        self.random_matrix = nn.Parameter(torch.empty(seq_length-1, low_dim))\n",
    "\n",
    "        # Initialize MLPs for processing elements of `envs_fourier`\n",
    "        self.mlps = nn.ModuleList([self._make_mlp(hidden_dim) for _ in range(low_dim)])\n",
    "\n",
    "        # Additional learnable coefficients for output computation\n",
    "        self.coeffs_attended = nn.Parameter(torch.empty(seq_length, input_dim))\n",
    "        self.offset = nn.Parameter(torch.randn(1))\n",
    "\n",
    "        # Initialize model parameters\n",
    "        self.init_parameters()\n",
    "\n",
    "    def _make_mlp(self, hidden_dim):\n",
    "        \"\"\"Utility function to create a simple MLP with one hidden layer.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"Initialize model parameters with appropriate distributions.\"\"\"\n",
    "        init_scale = 0.04\n",
    "\n",
    "        # Initialize attention matrices and additional parameters\n",
    "        for param in [self.query_matrix_1, self.key_matrix_1, self.value_matrix_1,\n",
    "                      self.query_matrix_2, self.key_matrix_2, self.value_matrix_2,\n",
    "                      self.query_matrix_3, self.key_matrix_3, self.value_matrix_3,\n",
    "                      self.random_matrix, self.coeffs_attended, self.offset]:\n",
    "            init.normal_(param, std=init_scale)\n",
    "\n",
    "        # Initialize weights and biases in MLPs\n",
    "        for mlp in self.mlps:\n",
    "            for layer in mlp:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    layer.bias.data.fill_(0)\n",
    "\n",
    "    def process_envs_fourier(self, envs_fourier):\n",
    "        \"\"\"\n",
    "        Process each element of envs_fourier using its respective MLP.\n",
    "\n",
    "        Args:\n",
    "            envs_fourier (Tensor): Input environmental Fourier embeddings of shape (batch_size, low_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Processed environmental features of shape (batch_size, low_dim).\n",
    "        \"\"\"\n",
    "        batch_size, _ = envs_fourier.shape\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(self.low_dim):  \n",
    "            input_column = envs_fourier[:, i].unsqueeze(1)  # Extract single column\n",
    "            output_column = self.mlps[i](input_column)  # Pass through corresponding MLP\n",
    "            outputs.append(output_column)\n",
    "        \n",
    "        processed_envs = torch.cat(outputs, dim=1)  \n",
    "        return processed_envs\n",
    "\n",
    "    def forward(self, x, envs_fourier):\n",
    "        \"\"\"\n",
    "        Forward pass through the three-layer attention model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_length-1, input_dim).\n",
    "            envs_fourier (Tensor): Environmental Fourier embeddings of shape (batch_size, low_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Final output prediction of shape (batch_size,).\n",
    "        \"\"\"\n",
    "\n",
    "        # Process `envs_fourier` using MLPs\n",
    "        mlp_output = self.process_envs_fourier(envs_fourier)\n",
    "\n",
    "        # Apply transformation using the learnable `random_matrix`\n",
    "        y = torch.matmul(x, self.random_matrix)\n",
    "\n",
    "        # Concatenate transformed input with MLP outputs\n",
    "        z = torch.cat((y, mlp_output.unsqueeze(1)), dim=1)\n",
    "\n",
    "        # Append ones to `z` for bias handling\n",
    "        z = torch.cat((z, torch.ones(z.shape[0], z.shape[1], 1).to(device)), dim=2)\n",
    "\n",
    "        # First attention layer\n",
    "        query_1 = torch.matmul(z, self.query_matrix_1)\n",
    "        key_1 = torch.matmul(z, self.key_matrix_1)\n",
    "        value_1 = torch.matmul(z, self.value_matrix_1)\n",
    "        scores_1 = torch.matmul(query_1, key_1.transpose(1, 2)).softmax(dim=-1)\n",
    "        attended_values_1 = torch.matmul(scores_1, value_1)\n",
    "\n",
    "        # Second attention layer\n",
    "        query_2 = torch.matmul(attended_values_1, self.query_matrix_2)\n",
    "        key_2 = torch.matmul(attended_values_1, self.key_matrix_2)\n",
    "        value_2 = torch.matmul(attended_values_1, self.value_matrix_2)\n",
    "        scores_2 = torch.matmul(query_2, key_2.transpose(1, 2)).softmax(dim=-1)\n",
    "        attended_values_2 = torch.matmul(scores_2, value_2)\n",
    "\n",
    "        # Third attention layer\n",
    "        query_3 = torch.matmul(attended_values_2, self.query_matrix_3)\n",
    "        key_3 = torch.matmul(attended_values_2, self.key_matrix_3)\n",
    "        value_3 = torch.matmul(attended_values_2, self.value_matrix_3)\n",
    "        scores_3 = torch.matmul(query_3, key_3.transpose(1, 2)).softmax(dim=-1)\n",
    "        attended_values_3 = torch.matmul(scores_3, value_3)\n",
    "\n",
    "        # Compute final output using learned coefficients and offset\n",
    "        output = torch.einsum(\"bij,ij->b\", attended_values_3, self.coeffs_attended) + self.offset\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01845ae0-5e35-47b9-afcb-d3a832346ede",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define function for fourier embedding of the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8012e5f3-7a83-47a4-8fba-66b2e19515b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_positional_embeddings(envs, d_model):\n",
    "    \"\"\"\n",
    "    Generate Fourier positional embeddings for a 1D tensor of positions.\n",
    "\n",
    "    Args:\n",
    "    - envs (Tensor): A 1D tensor of positions.\n",
    "    - d_model (int): The dimensionality of the embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: A 2D tensor of shape [len(envs), d_model] containing Fourier positional embeddings.\n",
    "    \"\"\"\n",
    "    position = envs.unsqueeze(1)\n",
    "    # Compute the div term\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(100.0)) / d_model))\n",
    "    # Apply sine to even indices in the tensor; 2i\n",
    "    pe_sin = torch.sin(position * div_term)\n",
    "    # Apply cosine to odd indices; 2i+1\n",
    "    pe_cos = torch.cos(position * div_term)\n",
    "\n",
    "    # interleave pe_sin and pe_cos\n",
    "    pe = torch.empty((len(envs), d_model), device=envs.device)\n",
    "    pe[:, 0::2] = pe_sin\n",
    "    pe[:, 1::2] = pe_cos if d_model % 2 == 0 else pe_cos[:, :-1] \n",
    "\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e67fd-c0cf-43ff-b75f-6c54c4aa289b",
   "metadata": {},
   "source": [
    "### Training on all temperatures combined except 29 and 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d95168-0739-4d4c-a2cd-06cf1ae35731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the directory for saving results\n",
    "save_dir = \"./synthetic_data_multi_env_attention_interpol\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Define the output file for storing test R2 scores\n",
    "filename2 = f\"{save_dir}/test_r2_score_vs_epsilon.txt\"\n",
    "if os.path.exists(filename2):\n",
    "    os.remove(filename2)  # Remove existing file if present\n",
    "\n",
    "def write_to_file(filename, *args):\n",
    "    \"\"\"Writes space-separated values to a file.\"\"\"\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(' '.join(map(str, args)) + '\\n')\n",
    "\n",
    "def generate_noise(shape, mean, std_dev):\n",
    "    \"\"\"Generates Gaussian noise with given mean and standard deviation.\"\"\"\n",
    "    return np.random.normal(mean, std_dev, shape)\n",
    "\n",
    "def generate_interactions(geno_data, order, num_combinations):\n",
    "    \"\"\"\n",
    "    Generates `order`-order interaction terms for genotype data with a specified number of unique `order`-way combinations.\n",
    "\n",
    "    Args:\n",
    "        geno_data (np.array): The original genotype data, shape = (n_samples, n_loci).\n",
    "        order (int): The interaction order (e.g., 2 for pairwise, 4 for fourth-order).\n",
    "        num_combinations (int): The number of unique random `order` combinations to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Interaction terms, shape = (n_samples, num_combinations).\n",
    "    \"\"\"\n",
    "    n_samples, n_loci = geno_data.shape\n",
    "    interaction_terms = np.empty((n_samples, num_combinations))\n",
    "\n",
    "    # Track selected combinations to ensure uniqueness\n",
    "    selected_combinations = set()\n",
    "    \n",
    "    i = 0\n",
    "    while i < num_combinations:\n",
    "        # Sample `order` random loci without replacement\n",
    "        loci_indices = tuple(sorted(np.random.choice(n_loci, size=order, replace=False)))\n",
    "        \n",
    "        # Ensure uniqueness of combinations\n",
    "        if loci_indices not in selected_combinations:\n",
    "            interaction_terms[:, i] = np.prod(geno_data[:, loci_indices], axis=1)  # Compute interaction product\n",
    "            selected_combinations.add(loci_indices)\n",
    "            i += 1\n",
    "\n",
    "    return interaction_terms\n",
    "\n",
    "# Constants for generating synthetic fitness data\n",
    "mean1 = 0.5\n",
    "std1 = 0.5\n",
    "scale = 1e-2\n",
    "t0 = 30.0  # Reference temperature\n",
    "\n",
    "def calculate_fitness_with_interactions(geno_data, temps, num_loci, e, order):\n",
    "    \"\"\"\n",
    "    Calculates fitness scores based on genotype data with noise, including:\n",
    "      - Constant genetic effects\n",
    "      - Linear effects\n",
    "      - Higher-order (4th-order) interaction effects\n",
    "\n",
    "    Args:\n",
    "        geno_data (np.array): Genotype data of shape (n_samples, num_loci).\n",
    "        temps (np.array): Array of environmental temperatures.\n",
    "        num_loci (int): Number of loci considered.\n",
    "        e (float): Proportion of linear vs interaction contributions (e=1 → all linear, e=0 → all interactions).\n",
    "        order (int): Interaction order (e.g., 4 for fourth-order interactions).\n",
    "\n",
    "    Returns:\n",
    "        np.array: Fitness values with added noise, shape = (n_samples, len(temps)).\n",
    "    \"\"\"\n",
    "    np.random.seed(0)  # Set seed for reproducibility\n",
    "    num_temps = temps.shape[0]\n",
    "\n",
    "    # Generate random coefficients for constant, linear, and square terms for each locus\n",
    "    coeffs_const = np.random.normal(mean1, std1, num_loci)  \n",
    "    coeffs_lin = np.random.normal(mean1, std1, num_loci)  \n",
    "    coeffs_square = np.random.normal(mean1, std1, num_loci)\n",
    "\n",
    "    # Generate higher-order interaction data\n",
    "    interaction_data = generate_interactions(geno_data, order, num_loci)\n",
    "\n",
    "    # Generate random coefficients for interactions\n",
    "    interaction_coeffs_const = np.random.normal(mean1, std1, interaction_data.shape[1])  \n",
    "    interaction_coeffs_lin = np.random.normal(mean1, std1, interaction_data.shape[1]) \n",
    "    interaction_coeffs_square = np.random.normal(mean1, std1, interaction_data.shape[1]) \n",
    "\n",
    "    # Generate random offset for the fitness function\n",
    "    offset = np.random.normal(0, 1, 1)  \n",
    "\n",
    "    # Initialize fitness matrix\n",
    "    fitness = np.zeros((geno_data.shape[0], num_temps))\n",
    "\n",
    "    # Compute fitness across all temperature conditions\n",
    "    for i, temp in enumerate(temps):\n",
    "        # Compute linear and interaction-based terms using quadratic temperature dependency\n",
    "        linear_terms = scale * np.dot(\n",
    "            geno_data, \n",
    "            coeffs_square * (temp - t0) ** 2 + coeffs_lin * (temp - t0) + coeffs_const\n",
    "        ) / num_loci\n",
    "\n",
    "        interaction_terms = scale * np.dot(\n",
    "            interaction_data, \n",
    "            interaction_coeffs_square * (temp - t0) ** 2 + interaction_coeffs_lin * (temp - t0) + interaction_coeffs_const\n",
    "        ) / interaction_data.shape[1]\n",
    "\n",
    "        # Compute fitness: weighted sum of linear and interaction effects, plus offset\n",
    "        y = e * linear_terms + (1 - e) * interaction_terms + scale * offset\n",
    "\n",
    "        # Apply transformation to ensure fitness values remain positive\n",
    "        fitness[:, i] = -1.0 * y + 1.0  \n",
    "\n",
    "    # Add Gaussian noise to fitness data\n",
    "    fitness_with_noise = np.zeros(fitness.shape)\n",
    "    for i, temp in enumerate(temps):\n",
    "        # Compute noise standard deviation as 20% of the standard deviation of fitness\n",
    "        std = 0.2 * np.mean(np.var(fitness[:, i]) ** 0.5)\n",
    "        noise = generate_noise(fitness.shape[0], 0.0, std)\n",
    "        fitness_with_noise[:, i] = fitness[:, i] + noise\n",
    "\n",
    "    return fitness_with_noise\n",
    "\n",
    "# Define list of e values controlling interaction contribution\n",
    "e_list = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# Generate synthetic genotype data\n",
    "genotype = pick_loci(num_loci)  # Function to select loci (not defined in script)\n",
    "\n",
    "for e in e_list:\n",
    "    # Generate synthetic fitness data with noise\n",
    "    fitness_with_noise = calculate_fitness_with_interactions(genotype, temps, num_loci, e, 4)\n",
    "\n",
    "    # Reduce the fitness dataset to selected temperature indices\n",
    "    fitness_reduced = fitness_with_noise[:, [0, 1, 2, 4, 5, 7]]\n",
    "\n",
    "    # Define model hyperparameters\n",
    "    low_dim = 30\n",
    "    seq_length = num_loci + 1\n",
    "    input_dim = low_dim + 1\n",
    "    query_dim = low_dim + 1\n",
    "    key_dim = low_dim + 1\n",
    "    hidden_dim = 32\n",
    "\n",
    "    # Define filename for storing validation R² scores\n",
    "    filename = f\"{save_dir}/validation_r2.txt\"\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "\n",
    "    # Determine computing device (CUDA if available)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    # Split data into training, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(genotype, fitness_reduced, test_size=0.15, random_state=42)\n",
    "    X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "    # Normalize fitness values based on training mean and standard deviation\n",
    "    mean_values = np.nanmean(y_train2, axis=0)\n",
    "    std_values = np.nanstd(y_train2, axis=0)\n",
    "    y_train2 = (y_train2 - mean_values) / std_values\n",
    "    y_val = (y_val - mean_values) / std_values\n",
    "    y_test = (y_test - mean_values) / std_values\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tens = torch.tensor(X_train2).float()\n",
    "    y_train_tens = torch.tensor(np.array(y_train2)).float()\n",
    "\n",
    "    # Initialize the attention model and move it to the appropriate device\n",
    "    attention_layer = ThreeLayerAttention(input_dim, query_dim, key_dim, seq_length).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(attention_layer.parameters(), lr=0.001)\n",
    "\n",
    "    # Define environment temperature settings\n",
    "    temps2 = np.array([23.0, 25.0, 27.0, 31.0, 33.0, 37.0])\n",
    "    num_temps2 = temps2.shape[0]\n",
    "    num_elements = int(64 / num_temps2)  # Number of elements per batch per environment\n",
    "    batch_size = num_temps2 * num_elements  # Define batch size\n",
    "    chunk_size = 100  # Chunk size for validation processing\n",
    "    num_epochs = 500  # Total number of training epochs\n",
    "    num_batches = X_train_tens.size(0) // batch_size  # Number of batches per epoch\n",
    "\n",
    "    ### **TRAINING LOOP**\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle training data before each epoch\n",
    "        indices = torch.randperm(X_train_tens.size(0))\n",
    "        train_input_shuffled = X_train_tens[indices]\n",
    "        train_target_shuffled = y_train_tens[indices].T  # Transpose for easier environment access\n",
    "\n",
    "        # Mini-batch training loop\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "\n",
    "            # Collect corresponding fitness data for each environment\n",
    "            fitness_env = []\n",
    "            for j, row in enumerate(train_target_shuffled):\n",
    "                start_col = start_idx + j * num_elements\n",
    "                end_col = start_col + num_elements\n",
    "                fitness_env.extend(row[start_col:end_col])\n",
    "\n",
    "            mini_batch_input = train_input_shuffled[start_idx:end_idx].to(device)\n",
    "            mini_batch_target = torch.tensor(fitness_env).to(device)\n",
    "\n",
    "            # Generate environment embeddings\n",
    "            envs = temps2.repeat(num_elements)\n",
    "            envs = torch.tensor(envs)\n",
    "            envs_fourier = fourier_positional_embeddings(envs, low_dim).to(device)\n",
    "\n",
    "            # Create one-hot encoded input representation\n",
    "            one_hot_mini_batch_input = torch.zeros((mini_batch_input.shape[0], num_loci, num_loci), device=device)\n",
    "            indices = torch.arange(num_loci, device=device)\n",
    "            one_hot_mini_batch_input[:, indices, indices] = mini_batch_input.squeeze()\n",
    "\n",
    "            # Forward pass and loss computation\n",
    "            train_output = attention_layer(one_hot_mini_batch_input, envs_fourier)\n",
    "            train_loss = loss_function(train_output, mini_batch_target)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Save model state after each epoch\n",
    "        model_state_path = os.path.join(save_dir, f\"epoch_{epoch}.pt\")\n",
    "        torch.save(attention_layer.state_dict(), model_state_path)\n",
    "\n",
    "        ### **VALIDATION PHASE**\n",
    "        y_pred = torch.tensor([]).to(device)\n",
    "        y_val_all = np.array([])\n",
    "\n",
    "        for env in range(num_temps2):\n",
    "            y_val_env = y_val.T[env]  # Extract validation target values\n",
    "\n",
    "            # Convert validation genotype data to PyTorch tensors\n",
    "            X_val_tens = torch.tensor(np.array(X_val)).float().to(device)\n",
    "\n",
    "            # Process validation data in chunks to avoid memory overflow\n",
    "            for i in range(0, len(X_val_tens), chunk_size):\n",
    "                chunk = X_val_tens[i:i + chunk_size].to(device)\n",
    "                chunk_size_actual = min(chunk_size, len(X_val_tens) - i)\n",
    "\n",
    "                envs = [temps2[env] for _ in range(chunk_size_actual)]\n",
    "                envs = torch.tensor(envs)\n",
    "                envs_fourier = fourier_positional_embeddings(envs, low_dim).to(device)\n",
    "\n",
    "                one_hot_val_input = torch.zeros((chunk_size_actual, num_loci, num_loci), device=device)\n",
    "                indices = torch.arange(num_loci, device=device)\n",
    "                one_hot_val_input[:, indices, indices] = chunk.squeeze(dim=1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    i_pred = attention_layer(one_hot_val_input, envs_fourier)\n",
    "\n",
    "                y_pred = torch.cat((y_pred, i_pred), dim=0)\n",
    "            y_val_all = np.concatenate((y_val_all, y_val_env))\n",
    "\n",
    "        # Compute validation R² score\n",
    "        val_r_squared = r2_score(y_val_all, y_pred.cpu())\n",
    "\n",
    "        # Save validation results\n",
    "        write_to_file(filename, low_dim, epoch, val_r_squared)\n",
    "        torch.cuda.empty_cache()  # Free up memory\n",
    "\n",
    "    ### **MODEL SELECTION: CHOOSE BEST EPOCH BASED ON VALIDATION PERFORMANCE**\n",
    "    data = pd.read_csv(filename, sep='\\s+', header=None)\n",
    "    max_row_index = data[2].idxmax()  # Find the index of the max R² score\n",
    "    max_row = data.loc[max_row_index]  # Extract the row with the best epoch\n",
    "    best_epoch = int(max_row[1])  # Extract best epoch number\n",
    "\n",
    "    # Copy the best-performing model to a dedicated file\n",
    "    model_path = f\"{save_dir}/epoch_{best_epoch}.pt\"\n",
    "    shutil.copyfile(model_path, f\"{save_dir}/best_model_e_{e}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b846786-d8e7-4207-914f-525da48cf226",
   "metadata": {},
   "source": [
    "### Test evaluation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c934a-e82f-41eb-8bb3-e80152a52fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the directory for saving results\n",
    "save_dir = \"./synthetic_data_multi_env_attention_interpol\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Define the output filename for storing R² scores vs epsilon\n",
    "filename2 = f\"{save_dir}/test_r2_score_vs_epsilon.txt\"\n",
    "\n",
    "# Remove the file if it already exists to start fresh\n",
    "if os.path.exists(filename2):\n",
    "    os.remove(filename2)\n",
    "\n",
    "def write_to_file(filename, *args):\n",
    "    \"\"\"\n",
    "    Append values to a file, separated by spaces.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the file where data should be written.\n",
    "        *args: The values to be written in the file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(' '.join(map(str, args)) + '\\n')  # Convert arguments to strings and join with spaces\n",
    "\n",
    "def generate_noise(shape, mean, std_dev):\n",
    "    \"\"\"\n",
    "    Generates Gaussian noise for data augmentation or randomness.\n",
    "\n",
    "    Args:\n",
    "        shape (tuple): The shape of the noise array.\n",
    "        mean (float): The mean of the Gaussian distribution.\n",
    "        std_dev (float): The standard deviation of the Gaussian distribution.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array of Gaussian noise with the specified shape.\n",
    "    \"\"\"\n",
    "    return np.random.normal(mean, std_dev, shape)  # Generate normally distributed random values\n",
    "\n",
    "def generate_interactions(geno_data, order, num_combinations):\n",
    "    \"\"\"\n",
    "    Generates interaction terms of the specified order for genotype data.\n",
    "\n",
    "    Args:\n",
    "        geno_data (np.ndarray): Genotype data of shape (n_samples, n_loci).\n",
    "        order (int): The interaction order (e.g., 2 for pairwise, 4 for fourth-order).\n",
    "        num_combinations (int): Number of unique random 'order' interactions to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Interaction terms, shape (n_samples, num_combinations).\n",
    "    \"\"\"\n",
    "    n_samples, n_loci = geno_data.shape  # Get number of samples and loci\n",
    "    interaction_terms = np.empty((n_samples, num_combinations))  # Initialize empty array\n",
    "\n",
    "    # Keep track of selected combinations to ensure uniqueness\n",
    "    selected_combinations = set()\n",
    "\n",
    "    i = 0  # Counter for filling interaction terms\n",
    "    while i < num_combinations:\n",
    "        # Randomly select `order` loci indices without replacement\n",
    "        loci_indices = tuple(sorted(np.random.choice(n_loci, size=order, replace=False)))\n",
    "\n",
    "        # Ensure uniqueness: only add if this combination hasn't been used before\n",
    "        if loci_indices not in selected_combinations:\n",
    "            # Compute the interaction term as the product of the selected loci's values\n",
    "            interaction_terms[:, i] = np.prod(geno_data[:, loci_indices], axis=1)\n",
    "            selected_combinations.add(loci_indices)  # Add combination to the set\n",
    "            i += 1  # Move to the next combination\n",
    "\n",
    "    return interaction_terms  # Return the generated interaction terms\n",
    "\n",
    "# Define parameters for fitness calculation\n",
    "mean1 = 0.5  # Mean value for randomly generated coefficients\n",
    "std1 = 0.5   # Standard deviation for randomly generated coefficients\n",
    "scale = 1e-2  # Scaling factor for fitness contributions\n",
    "t0 = 30.0  # Reference temperature for temperature-dependent terms\n",
    "\n",
    "def calculate_fitness_with_interactions(geno_data, temps, num_combinations, e, order):\n",
    "    \"\"\"\n",
    "    Calculates fitness scores based on genotype data with added noise.\n",
    "    The function includes:\n",
    "      - Constant genetic effects\n",
    "      - Linear effects\n",
    "      - Higher-order (e.g., 4th-order) interactions\n",
    "\n",
    "    Args:\n",
    "        geno_data (np.ndarray): Genotype data of shape (n_samples, n_loci).\n",
    "        temps (np.ndarray): Array of environmental temperatures.\n",
    "        num_combinations (int): Number of unique random interaction combinations.\n",
    "        e (float): Proportion of linear vs interaction contributions (e=1 → all linear, e=0 → all interactions).\n",
    "        order (int): Interaction order (e.g., 4 for fourth-order interactions).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Fitness values with added noise, shape (n_samples, len(temps)).\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(0)  # Set seed for reproducibility\n",
    "    \n",
    "    num_temps = temps.shape[0]  # Number of environmental conditions (temperatures)\n",
    "\n",
    "    # Generate random coefficients for constant, linear, and quadratic genetic effects\n",
    "    coeffs_const = np.random.normal(mean1, std1, num_loci)  # Constant effects\n",
    "    coeffs_lin = np.random.normal(mean1, std1, num_loci)  # Linear effects\n",
    "    coeffs_square = np.random.normal(mean1, std1, num_loci)  # Quadratic effects\n",
    "\n",
    "    # Generate higher-order interaction data from genotype data\n",
    "    interaction_data = generate_interactions(geno_data, order, num_combinations)\n",
    "\n",
    "    # Generate random coefficients for interaction effects\n",
    "    interaction_coeffs_const = np.random.normal(mean1, std1, interaction_data.shape[1])  # Constant interaction effects\n",
    "    interaction_coeffs_lin = np.random.normal(mean1, std1, interaction_data.shape[1])  # Linear interaction effects\n",
    "    interaction_coeffs_square = np.random.normal(mean1, std1, interaction_data.shape[1])  # Quadratic interaction effects\n",
    "\n",
    "    # Generate a random offset term for baseline fitness adjustment\n",
    "    offset = np.random.normal(0, 1, 1)\n",
    "\n",
    "    # Initialize fitness array with zeros, shape: (n_samples, num_temps)\n",
    "    fitness = np.zeros((geno_data.shape[0], num_temps))\n",
    "\n",
    "    # Compute fitness values for each temperature condition\n",
    "    for i, temp in enumerate(temps):\n",
    "        # Compute linear genetic effects, with quadratic dependency on temperature\n",
    "        linear_terms = scale * np.dot(\n",
    "            geno_data, \n",
    "            coeffs_square * (temp - t0)**2 + coeffs_lin * (temp - t0) + coeffs_const\n",
    "        ) / num_loci  # Normalize by num_loci\n",
    "\n",
    "        # Compute interaction effects, also temperature-dependent\n",
    "        interaction_terms = scale * np.dot(\n",
    "            interaction_data, \n",
    "            interaction_coeffs_square * (temp - t0)**2 + interaction_coeffs_lin * (temp - t0) + interaction_coeffs_const\n",
    "        ) / interaction_data.shape[1]  # Normalize by number of interaction terms\n",
    "\n",
    "        # Compute weighted sum of linear and interaction effects\n",
    "        y = e * linear_terms + (1 - e) * interaction_terms + scale * offset\n",
    "\n",
    "        # Adjust fitness values so that higher fitness is represented by positive values\n",
    "        fitness[:, i] = -1.0 * y + 1.0  \n",
    "\n",
    "    # Add Gaussian noise to the fitness data\n",
    "    fitness_with_noise = np.zeros(fitness.shape)\n",
    "\n",
    "    for i, temp in enumerate(temps):\n",
    "        # Compute noise standard deviation as 20% of the standard deviation of fitness\n",
    "        std = 0.2 * np.mean(np.var(fitness[:, i])**0.5)\n",
    "        \n",
    "        # Generate noise for the fitness values\n",
    "        noise = generate_noise(fitness.shape[0], 0.0, std)\n",
    "        \n",
    "        # Add noise to the original fitness values\n",
    "        fitness_with_noise[:, i] = fitness[:, i] + noise\n",
    "\n",
    "    return fitness_with_noise  # Return the noisy fitness matrix\n",
    "\n",
    "# Define list of epsilon values controlling interaction contribution\n",
    "e_list = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# Generate genotype data by selecting loci (function pick_loci not defined in this snippet)\n",
    "genotype = pick_loci(num_loci)\n",
    "\n",
    "# Define environmental temperature conditions\n",
    "temps = np.array([23.0, 25.0, 27.0, 29.0, 31.0, 33.0, 35.0, 37.0])\n",
    "\n",
    "# Iterate over different values of e (proportion of linear vs. interaction effects)\n",
    "for row, e in enumerate(e_list):\n",
    "\n",
    "    # Generate synthetic fitness data with noise based on interactions\n",
    "    fitness_with_noise = calculate_fitness_with_interactions(genotype, temps, num_loci, e, 4)\n",
    "\n",
    "    # Define model hyperparameters\n",
    "    low_dim = 30  # Dimensionality of the Fourier embeddings\n",
    "    seq_length = num_loci + 1  # Sequence length\n",
    "    input_dim = low_dim + 1  # Input dimension\n",
    "    query_dim = low_dim + 1  # Query vector dimension\n",
    "    key_dim = low_dim + 1  # Key vector dimension\n",
    "    hidden_dim = 32  # Hidden layer size for the MLPs\n",
    "    chunk_size = 100  # Number of samples processed per batch in validation/testing\n",
    "\n",
    "    # Split the dataset into training, validation, and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(genotype, fitness_with_noise, test_size=0.15, random_state=42)\n",
    "    X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "    # Compute the mean and standard deviation for normalization\n",
    "    mean_values = np.nanmean(y_train2, axis=0)\n",
    "    std_values = np.nanstd(y_train2, axis=0)\n",
    "\n",
    "    # Select all columns except the 5th (index 3) and 7th (index 6) for normalization\n",
    "    cols_to_normalize = [i for i in range(y_train2.shape[1]) if i not in [3, 6]]\n",
    "\n",
    "    # Normalize y_train2, y_val, y_test excluding the 5th and 7th columns\n",
    "    y_train2[:, cols_to_normalize] = (y_train2[:, cols_to_normalize] - mean_values[cols_to_normalize]) / std_values[cols_to_normalize]\n",
    "    y_val[:, cols_to_normalize] = (y_val[:, cols_to_normalize] - mean_values[cols_to_normalize]) / std_values[cols_to_normalize]\n",
    "    y_test[:, cols_to_normalize] = (y_test[:, cols_to_normalize] - mean_values[cols_to_normalize]) / std_values[cols_to_normalize]\n",
    "\n",
    "    # Set computing device (CUDA if available, else CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the pre-trained attention model\n",
    "    attention_layer = ThreeLayerAttention(input_dim, query_dim, key_dim, seq_length).to(device)\n",
    "    model_path = f\"{save_dir}/best_model_e_{e}.pt\"\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    attention_layer.load_state_dict(state_dict)\n",
    "    attention_layer.to(device)  # Move model to appropriate device\n",
    "    attention_layer.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Load the mean and standard deviation values used for fitness scaling\n",
    "    mean_data = np.load('./synthetic_data_multi_env_attention_interpol/fitting_mean_fitness.npy')\n",
    "    std_data = np.load('./synthetic_data_multi_env_attention_interpol/fitting_std_fitness.npy')\n",
    "\n",
    "    # Iterate over all environmental temperature conditions\n",
    "    for env in range(temps.shape[0]):\n",
    "\n",
    "        y_pred = torch.tensor([]).to(device)  # Initialize tensor for storing predictions\n",
    "        y_test_env = y_test.T[env]  # Extract the test set fitness values for the current environment\n",
    "\n",
    "        # Convert test genotype data to PyTorch tensor\n",
    "        X_test_tens = torch.tensor(np.array(X_test)).float().to(device)\n",
    "\n",
    "        # Process test samples in chunks to avoid memory overflow\n",
    "        for i in range(0, len(X_test_tens), chunk_size):\n",
    "            chunk = X_test_tens[i:i + chunk_size].to(device)\n",
    "\n",
    "            # Determine the actual chunk size for the last batch\n",
    "            chunk_size_actual = min(chunk_size, len(X_test_tens) - i)\n",
    "\n",
    "            # Generate environment embeddings for the current chunk\n",
    "            envs = [temps[env] for _ in range(chunk_size_actual)]\n",
    "            envs = torch.tensor(envs)\n",
    "            envs_fourier = fourier_positional_embeddings(envs, low_dim).to(device)\n",
    "\n",
    "            # Create one-hot encoded input representation for loci\n",
    "            one_hot_test_input = torch.zeros((chunk_size_actual, num_loci, num_loci), device=device)\n",
    "            indices = torch.arange(num_loci, device=device)\n",
    "            one_hot_test_input[:, indices, indices] = chunk.squeeze(dim=1)\n",
    "\n",
    "            # Perform model inference without computing gradients\n",
    "            with torch.no_grad():\n",
    "                i_pred = attention_layer(one_hot_test_input, envs_fourier)\n",
    "\n",
    "            # Rescale predictions for specific environments (5th and 7th temperatures)\n",
    "            if env == 3:\n",
    "                y_pred = torch.cat((y_pred, i_pred * std_data[row, 0] + mean_data[row, 0]), dim=0)\n",
    "            elif env == 6:\n",
    "                y_pred = torch.cat((y_pred, i_pred * std_data[row, 1] + mean_data[row, 1]), dim=0)\n",
    "            else:\n",
    "                y_pred = torch.cat((y_pred, i_pred), dim=0)\n",
    "\n",
    "        # Compute R² score for test predictions\n",
    "        test_r_squared = r2_score(y_test_env, y_pred.cpu())\n",
    "\n",
    "        # Save test R² score along with epsilon and temperature\n",
    "        write_to_file(filename2, e, temps[env], test_r_squared)\n",
    "\n",
    "        # Print results for monitoring\n",
    "        print(temps[env], e, test_r_squared)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
