{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcae895-c7c6-4681-8a79-56a486271448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e185b78-ee10-4c8f-ab06-9d8f83381f40",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load genotype and fitness experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae7bf68-85fa-4945-a755-6c923fd06fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load genotype data, converting to a numeric format where allele states are mapped to {-1, 1}\n",
    "geno_data = 2.0 * np.load('merged_geno_data.npy')[:, 1:] - 1.0  \n",
    "\n",
    "# Load phenotype data from a tab-separated file\n",
    "df_pheno23C = pd.read_csv('pheno_data_23C.txt', sep=\"\\t\")\n",
    "\n",
    "# Get the total number of segregants (samples)\n",
    "num_segregants = geno_data.shape[0]\n",
    "\n",
    "# Create a list of indices and shuffle them for randomization\n",
    "shuffled_indices = list(range(num_segregants))\n",
    "random.seed(0)  # Set a fixed seed to ensure reproducibility\n",
    "random.shuffle(shuffled_indices)\n",
    "\n",
    "# Extract fitness data from the phenotype dataframe (excluding the last row)\n",
    "data_fitness = np.array(df_pheno23C.iloc[0:df_pheno23C.shape[0]-1, 1])\n",
    "\n",
    "# Apply the shuffled indices to the genotype data to maintain correspondence\n",
    "geno_data = geno_data[shuffled_indices]\n",
    "\n",
    "# Shuffle the fitness data using the same indices to keep alignment with genotype data\n",
    "data_fitness = df_pheno23C.iloc[shuffled_indices, 1].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3dbc25-8856-41bc-8999-aa7359d41845",
   "metadata": {},
   "source": [
    "### Split the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7e7c032-bc0e-40ed-9c54-7e759e6c472d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training (85%) and testing (15%) sets\n",
    "# Ensures reproducibility with a fixed random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(geno_data, data_fitness, test_size=0.15, random_state=42)\n",
    "\n",
    "# Further split the training set into training (85%) and validation (15%) sets\n",
    "# This allows tuning model hyperparameters without touching the test set\n",
    "X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d5aae8-9c5d-464b-86ad-9b05cce838cd",
   "metadata": {},
   "source": [
    "### Obtain SNP correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373699b9-f630-467b-80cb-3477c7ab475c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define batch size for processing covariance in chunks\n",
    "# This helps manage memory usage efficiently when working with large datasets\n",
    "batch_size = 10000  # Adjust this based on your available memory\n",
    "\n",
    "# Detect and set computation device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert training data to a PyTorch tensor\n",
    "X_train = torch.tensor(X_train)\n",
    "print(1)  # Simple print statement for debugging purposes\n",
    "\n",
    "# Compute the mean of each column (feature-wise mean) and move it to the selected device\n",
    "mean = torch.mean(X_train, dim=0).to(device)\n",
    "\n",
    "# Initialize an empty covariance matrix of shape (num_features, num_features)\n",
    "covariance_matrix = torch.zeros(X_train.shape[1], X_train.shape[1])\n",
    "\n",
    "# Iterate over the dataset in mini-batches to compute covariance matrix efficiently\n",
    "for i in range(0, len(X_train), batch_size):\n",
    "    # Extract mini-batch and move it to the selected device\n",
    "    batch = X_train[i:i+batch_size].to(device)\n",
    "    \n",
    "    # Center the batch by subtracting the precomputed column means\n",
    "    centered_batch = batch - mean\n",
    "    \n",
    "    # Compute the outer product of the centered batch for covariance calculation\n",
    "    aa = torch.matmul(centered_batch.T, centered_batch)\n",
    "    \n",
    "    # Accumulate batch-wise covariance into the overall covariance matrix (move to CPU before summing)\n",
    "    covariance_matrix += aa.cpu()\n",
    "    \n",
    "    # Print batch index to track progress\n",
    "    print(i)\n",
    "    \n",
    "    # Delete temporary variable to free up memory\n",
    "    del aa\n",
    "\n",
    "# Normalize the accumulated covariance matrix to get an unbiased estimate\n",
    "covariance_matrix /= (X_train.shape[0] - 1)\n",
    "\n",
    "# Compute the standard deviations of each column (feature-wise std deviation)\n",
    "std_dev = torch.sqrt(torch.diag(covariance_matrix))\n",
    "\n",
    "# Calculate the correlation matrix by normalizing the covariance matrix\n",
    "# Each element is divided by the product of standard deviations of the corresponding variables\n",
    "correlation_matrix = covariance_matrix / torch.outer(std_dev, std_dev)\n",
    "\n",
    "# Move the correlation matrix back to CPU and convert it to a NumPy array for saving\n",
    "correlation_matrix = correlation_matrix.cpu().numpy()\n",
    "\n",
    "# Save the computed correlation matrix to a file for later use\n",
    "np.save(\"correlation_matrix.npy\", correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20cb33-746e-48f1-abbe-ca174430c56c",
   "metadata": {},
   "source": [
    "### Filter independent loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b3214c-1488-4f3c-8817-c6c2e5dbc952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set correlation threshold for filtering independent loci\n",
    "a = 0.94  \n",
    "\n",
    "# Function to identify independent causal loci based on correlation threshold\n",
    "def ind_causal_loci(correlation_matrix):\n",
    "    \"\"\"\n",
    "    Identifies a set of independent loci by selecting one representative locus \n",
    "    from each highly correlated group (> |a|).\n",
    "\n",
    "    Args:\n",
    "        correlation_matrix (numpy.ndarray): Square correlation matrix of loci.\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of selected independent loci.\n",
    "    \"\"\"\n",
    "    num_data_points = correlation_matrix.shape[0]  # Number of loci (features)\n",
    "    \n",
    "    selected_data_points = set()  # Set to store selected independent loci\n",
    "    remaining_data_points = set(range(num_data_points))  # Set of loci to evaluate\n",
    "\n",
    "    while remaining_data_points:\n",
    "        # Pick one locus from remaining set and mark it as selected\n",
    "        current_data_point = remaining_data_points.pop()\n",
    "        selected_data_points.add(current_data_point)\n",
    "\n",
    "        # Find all loci that are highly correlated with the selected locus\n",
    "        correlated_data_points = set(\n",
    "            i for i in remaining_data_points\n",
    "            if correlation_matrix[current_data_point, i] > a or correlation_matrix[current_data_point, i] < -a\n",
    "        )\n",
    "\n",
    "        # Remove correlated loci from the remaining set to avoid redundancy\n",
    "        remaining_data_points -= correlated_data_points\n",
    "\n",
    "    return list(selected_data_points)\n",
    "\n",
    "# Compute the list of independent loci using the defined function\n",
    "ind_loci_list = np.array(ind_causal_loci(correlation_matrix)) + 1  # Convert to NumPy array and adjust indexing\n",
    "\n",
    "# Save the independent loci list to a file for later use\n",
    "np.save('ind_loci_list_3.npy', ind_loci_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
